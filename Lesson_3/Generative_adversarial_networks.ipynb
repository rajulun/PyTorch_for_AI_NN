{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Generative adversarial networks.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXWA9T1wXtUc"
      },
      "source": [
        "1. Как было сказано ранее, GAN обучается воспроизводить реальные данные. Поэтому Вам предлагается обучить генератор создавать точки, которые будут лежать на графике функции $y = \\frac{sin(x)}{x} - \\frac{x}{10}$. При выполненинии данного задания структура GAN остается той же, но Вам нужно:\n",
        "        Сгенерировать настоящие данные\n",
        "        Изменить архитектуру дискриминатора и генератора\n",
        "        Без графиков домашку не принимаю (реальные данные + результат модели, графики двух лоссов)\n",
        "        Дополнительно: посмотреть зависимость от количества изначальных реальных данных (график)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yS9QfpEXtUw"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "########### Вывод изображений и сохранение ################################\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from torchvision.utils import save_image, make_grid\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hClrGYJaT2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f269d63-eda4-4227-f882-3ccb75086462"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "# device = torch.device(\"cpu\")\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZbIVHYv-eAi"
      },
      "source": [
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "lr = 0.001\n",
        "latent_dim = 2\n",
        "sample_interval = 100\n",
        "\n",
        "x_min, x_max = -8, 8\n",
        "x_len = 25600"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyawcQSyKFvk"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            layers.append(nn.ReLU())\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 16, normalize=False),\n",
        "            *block(16, 32),\n",
        "            *block(32, 16),\n",
        "            nn.Linear(16, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LRM5vrBCLjd"
      },
      "source": [
        "class CustomLinearLayer(nn.Module):\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        \n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights) \n",
        "\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        nn.init.uniform_(self.weights, -0.005, 0.005) \n",
        "        nn.init.uniform_(self.bias, -0.005, 0.005)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        w_times_x = torch.mm(x, self.weights.t())\n",
        "        return torch.add(w_times_x, self.bias)  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x3K5_jOKFr4"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        def block(in_feat, out_feat):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            layers.append(nn.LeakyReLU())\n",
        "            layers.append(nn.Dropout(0.3))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(2, 128),\n",
        "            *block(128, 256),\n",
        "            *block(256, 128),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5PlGN2KTjoq"
      },
      "source": [
        "def func_y(x):\n",
        "    return np.sin(x)/x - x/10\n",
        "# print(\"Func_y:\", func_y(x)) \n",
        "\n",
        "#данные два списка надо объединить в один вида data = [(x_0,y_0), (x_1, y_1),..] <- реальные данные "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9F1kouR_ioZ"
      },
      "source": [
        "x = np.linspace(x_min, x_max, x_len)\n",
        "y = func_y(x)\n",
        "\n",
        "train_data = torch.tensor(list(zip(x,func_y(x))))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o9VPfsAY_aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f03ac34-6216-406b-e395-b33096d08c10"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-8.0000,  0.9237],\n",
              "        [-7.9994,  0.9236],\n",
              "        [-7.9987,  0.9236],\n",
              "        ...,\n",
              "        [ 7.9987, -0.6762],\n",
              "        [ 7.9994, -0.6762],\n",
              "        [ 8.0000, -0.6763]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nM5iAXjQ8EG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "96e316de-e266-4543-de96-a2de1a8d26e0"
      },
      "source": [
        "plt.plot(x, y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1f5efaab00>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5dnH8e+dfSNkJQlJICxhSdiJIFC0igRQKurrgiu2tWpf625brW9tq7Zabau10ipFLVXrvoCKCiLusoR9hwBCWLKwBsie3O8fGWhEAoSZzJnJ3J/rmiszZ5nzA8Lcc55znucRVcUYY0zgCnI6gDHGGGdZITDGmABnhcAYYwKcFQJjjAlwVgiMMSbAhTgd4FQkJSVpVlaW0zGMMcavLFq0aJeqJh+93C8LQVZWFgUFBU7HMMYYvyIiW4613JqGjDEmwFkhMMaYAGeFwBhjApwVAmOMCXBWCIwxJsBZITDGmABnhcAYYwKcR/oRiMizwHigVFX7HGO9AH8FzgUqgGtVdbFr3STg/1ybPqiq0zyRyRhP23OohkVb9rJ1TwUHq+poFxFCl+RoBnWKp31kqNPxjDllnupQ9i/gSeDfzawfB2S7HkOBfwBDRSQB+A2QByiwSERmqOpeD+Uyxi2qyoerinlh3la+KNx1zG1CgoSzenXgx9/rwuldE72c0Bj3eaQQqOpnIpJ1nE0mAP/Wxllw5olInIikAd8HZqvqHgARmQ2MBV7yRC5j3LG0aB+/mb6SZdv2kx4XyS2jshmZnUT35BjaRYRQXlXH2uJyPl1XxhuLtzNxyjzO7tWBBy/oQ8e4SKfjG3PSvDXERDpQ1OT1Ntey5pZ/h4hcD1wP0KlTp9ZJaQxQV9/A5LkbeeLjDSTHhPOnS/pz4cB0goPkW9slRIcxvFsSw7slcfvoHvzrq294Ys4Gxj7+GY9e0p8xuakO/QmMaRm/uVisqlNUNU9V85KTvzNmkjEeUVFTxw3PL+Kxj9bzg35pzLrjDC4enPGdInC0iNBgbjyzGzNvGUmXpGhufGERUz/fhE0Fa/yBtwrBdiCzyesM17LmlhvjdXsO1XDZ0/OYu66UBybk8vjEgcRGtOwicFZSNK/cMIwxOak8+N4a/vZxYSulNcZzvFUIZgDXSKPTgf2quhP4EMgXkXgRiQfyXcuM8ar9FbVcNXU+60sO8M9r8rh6WNYpv1dEaDCTrxzERQPT+cvs9Uz9fJPnghrTCjx1++hLNF74TRKRbTTeCRQKoKpPATNpvHW0kMbbR3/oWrdHRB4AFrre6v7DF46N8ZZD1XVMem4BhaUHmXLNYL7fs4Pb7xkcJDxycT8qa+t58L01ZCZE2TUD47PEH9sw8/Ly1OYjMJ7Q0KDc+MIiPlpTwj+uGuzxD+uq2noue/prCksP8tZNI+iR0s6j729MS4jIIlXNO3q531wsNqY1/GX2ematLuH/zstplW/sEaHBPH11HlHhIdz4wiIqauo8fgxj3BVQhaCypt7u4jBHvLd8J0/OLWTiaZn8cERWqx0ntX0Ef504gM27DvGHmWta7TjGnKqAKgT3vrWCCZO/5J1lO6irb3A6jnFQ0Z4K7n5jOQMy47h/Qh8aR0FpPcO7JfGTkV15Yd5WPl5b0qrHMqalAqoQDO2awIGqOm5+aQlnPvoJrxYUUd9gZwiBpra+gVteXgLA3y4fSFiId/4b3Jnfg16p7fjVmys5WG1NRMZ3BFQhuOy0Tsy540ymXD2YpJgwfvH6cs574nO+bGYMGdM2PTFnA0u27uMPF/UlMyHKa8cNDwnmoYv6UnKgir/MWu+14xpzIgFVCACCgoT83FTevmkET14xkIqaeq6cOp+7XlvG/opap+OZVrZ82z4mzy3k4sEZ/KB/R68ff2CneK4c2ol/fbWZldv3e/34xhxLwBWCw0SE8f06Muv2M7jprG68tWQ75zz2KZ+tL3M6mmkltfUN/OL15STFhPPr8TmO5fj5mF4kRIdz3/SVdvOC8QkBWwgOiwgN5udjejH9phHER4Uy6bkF/GX2ert20AY9/elG1hYf4MEL+jg6f0D7yFDuyu/B4q37+GBlsWM5jDks4AvBYX3S2/P2TSO4aGAGT8zZwLXPLWB/pTUVtRWFpQd4Yk4h5/VLI98HevhePDiDHikx/PGDtdTU2R1sxllWCJqICgvhz5f25+GL+vL1xt1c8tRXbNtb4XQs4yZV5b7pq4gMC+a3P8h1Og4AIcFB3D2uF9/sruClBVudjmMCnBWCY5g4pBPTfjSEnfuruPDvX7Fqh13U82cfrCzmq427uTO/B8ntwp2Oc8RZPTswrGsiT8zZYD2OjaOsEDRjRPck3vzpcEKDhMunzGP5tn1ORzKnoLKmcdC3XqntuGKIb01oJCLcmd+D3Ydq+M98OyswzrFCcBzZKe145YZhxEaGcuXU+SzZalMp+5unPt3I9n2V/Pb8XEKCfe/XPS8rgRHdE3nq001U1dY7HccEKN/7n+FjMhOieOWGYSREh3H1MwtYsc2aifxF0Z4Knvp0Iz/o39GnJ5W/5exsdh2strMC4xgrBCchPS6SV64fRvvIUK59bgGbyg46HcmchEc+XIcI3DOul9NRjmto10SGdkngqU832lmBcYQVgpOU2j6CF64bCsDVzyygeH+Vw4nM8azYtp93lu3gJyO70jEu0uk4J3TLqGxKD1Tz9hKbqdV4nxWCFuiSFM20Hw1hf2Utk55dwIEq62fgq/74wVrio0K5/oyuTkc5KcO7JZKTFsvULzbTYJ0ZjZdZIWihPuntefrqwRSWHeTWl5daD2Qf9PmGMr4o3MXNZ2fTroWTzztFRPjJGV0oLD3IpxtsmBPjXVYITsGI7kn89vxcPl5byiMfrHU6jmmioUF5+P21ZMRHcuXpvnW76Imc17cjKbHhNtm98TorBKfo6tM7c/XpnXn6s028VlDkdBzj8s7yHazaUc5d+T0JDwl2Ok6LhIUEce3wLnxZuJvVO8qdjmMCiEcKgYiMFZF1IlIoIncfY/1jIrLU9VgvIvuarKtvsm6GJ/J4y30/yGFE90TufXulDSnsA2rqGvjTrHXkpMVyvgNDTHvCFUM6ERUWzDNfbHY6igkgbhcCEQkGJgPjgBzgchH51hi/qnq7qg5Q1QHA34A3m6yuPLxOVc93N483hQYH8cTEgSREhfG/Ly6m3C4eO+rVgiKK9lTyi7E9CQpq3aknW0v7qFAuGpTOu8t3sK+ixuk4JkB44oxgCFCoqptUtQZ4GZhwnO0vB17ywHF9QmJMOJOvHMiOfZX8/LVlNr68Q6rr6pk8t5BBneI4s0ey03HccuXQzlTXNfD6om1ORzEBwhOFIB1o2ki+zbXsO0SkM9AF+LjJ4ggRKRCReSJyQXMHEZHrXdsVlJX51l0Vgzsn8MuxvfhwVQnPfvmN03EC0qsLi9i5v4rbR/do9YnoW1vvtFgGd47nxflb7VZS4xXevlg8EXhdVZt2n+ysqnnAFcDjItLtWDuq6hRVzVPVvORk3/vGd93ILuTnpPDQzDU2DIWXVdXWM3nuRvI6x/O97klOx/GIq07vxOZdh/hq426no5gA4IlCsB3IbPI6w7XsWCZyVLOQqm53/dwEfAIM9EAmrxMRHrm4H0kx4dz6yhIqa2yoAG95ZWERxeVt42zgsHF90oiPCuWFeVucjmICgCcKwUIgW0S6iEgYjR/237n7R0R6AfHA102WxYtIuOt5EjACWO2BTI6Iiwrjz5f2Z1PZIf4wc43TcQJCVW09f/+kkCFZCQzv5rsDy7VURGgwl+RlMntNCSXlNpyJaV1uFwJVrQN+BnwIrAFeVdVVInK/iDS9C2gi8LJ++2pqb6BARJYBc4GHVdVvCwE0dja77ntdeH7eFuauLXU6Tpv30oKtlJRXc9vo7DZzNnDYFUM6Ud+g1k/FtDrxx7tc8vLytKCgwOkYzaqqreeCyV+y62ANH942ksQY35kVqy2pqq1n5CNz6ZoUzSs3DHM6Tqu49OmvKS2vYu5d329zhc54n4gscl2T/RbrWdwKIkKDeeyyAZRX1nLvWyudjtNmvTh/K2UHqrl9dA+no7SaSwZn8M3uCgq22KRIpvVYIWglvdNiuW10Nh+sKmbmip1Ox2lzKmvq+ccnGxnWNdGnJ51x17l904gKC+b1AutTYFqPFYJWdP3IrvRNb8+v317JnkPWS9STXpy/hV0Hq7ntnGyno7Sq6PAQzuubxrvLd9gE96bVWCFoRSHBQTx6ST/Kq2r53TurnI7TZlTV1vP0Z5sY1jWRoW34bOCwS/IyOVRTz/srip2OYtooKwStrFdqLD87K5vpS3cwe3WJ03HahFcWFlF2oJpbRrXts4HDTsuKJysxitcW2d1DpnVYIfCCn36/G71S23HvWyvYX2ED07mjuq7x2sCQrARO75rgdByvEBEuHpzBvE17KNpT4XQc0wZZIfCCsJAg/nRJf3YfquH3M/26m4TjXivYRnF5FbeManv9Bo7nwkEZADansWkVVgi8pE96e34ysiuvFmxj/iYbP+ZU1NQ18I9PNjKoUxwjurf9awNNpcdFMqRLAm8v3W4j3BqPs0LgRbeOyiYjPpJfvbWC6jobi6il3lqyje37Krk5wM4GDpswoCMbyw6xeqfNXmY8ywqBF0WGBfPABX3YWHaIKZ/avLQtUVvfwJNzC+mX0Z7v+/l8A6fq3D5phAQJM5bucDqKaWOsEHjZWT07ML5fGn+bW8jmXYecjuM3pi/dQdGeSm45OzDPBgDio8M4s0cyM5btsHkKjEdZIXDAfeNzCA8J4v/eXmHtvSehvkGZPLeQnLRYRvXu4HQcR50/oCM791ex4Js9TkcxbYgVAgd0iI3gF2N78WXhbqbbaf4Jvbt8B5t3HQq4O4WOZXROCpGhwfZ7YzzKCoFDrhzSiQGZcTzw7mqbpPw46huUv31cSM+UduTnpDgdx3FRYSHk56Ywc8VOauoanI5j2ggrBA4JChIeuqgv+yprefj9tU7H8Vnvr9xJYelBbh7VnaCgwD4bOGzCgI7sr6zls/W+NXe38V9WCBzUOy2W60Z24eWFRSzYbG2+R2toUP42p5DuHWIY1yfN6Tg+Y2R2MvFRocxYZs1DxjOsEDjs1lHZpMdFcu9bK+xU/yizVpewruQAN5/dnWA7GzgiNDiIMbmpzFlTQlWt9Ucx7rNC4LCosBAeuCCXDaUH+efn1rfgMFXliTkb6JoUzfh+HZ2O43PO7ZvGoZp6PrXmIeMBVgh8wNm9Uji3bypPzNnAlt3WtwDgw1XFrN5Zzk1n2dnAsQzrlkhcVKhNemQ8wgqBj/jND3IJDQ7i19NXBXzfgoYG5bHZG+iaHM0FA9OdjuOTQoODGJOTypw1pdY8ZNzmkUIgImNFZJ2IFIrI3cdYf62IlInIUtfjuibrJonIBtdjkify+KOU2Ajuyu/BZ+vLeGd5YH/Le3fFTtaVHOC2c3rY2cBxnNsvjYPVdXy+YZfTUYyfc7sQiEgwMBkYB+QAl4tIzjE2fUVVB7geU137JgC/AYYCQ4DfiEi8u5n81dXDsuiX0Z7731nN/srAnLegrr6Bxz9aT8+Udozva3cKHc/wbom0j7TmIeM+T5wRDAEKVXWTqtYALwMTTnLfMcBsVd2jqnuB2cBYD2TyS8FBwh8u7MueQ9U88kFg9i2YvnQHm8oOcfvoHtZv4ARCg4PIz0nho9UlNpqtcYsnCkE60HQOvW2uZUf7HxFZLiKvi0hmC/dFRK4XkQIRKSgra7t3SvRJb8+1w7vwnwVbWbx1r9NxvKq2voG/ztlAn/RYxuRaL+KTcW6/NA5U1/GFNQ8ZN3jrYvE7QJaq9qPxW/+0lr6Bqk5R1TxVzUtObtvDEN+R34PU2Ah+9eYKausDp2/BG4u2sXVPBXeM7hHwYwqdrBHdkoiNCOE9ax4ybvBEIdgOZDZ5neFadoSq7lbVatfLqcDgk903EMWEh/Db83NZW3yAZ7/Y7HQcr6iuq+dvHxcyIDOOs3oG9gijLREWEkR+biqzrXnIuMEThWAhkC0iXUQkDJgIzGi6gYg0vep3PrDG9fxDIF9E4l0XifNdywLemNxUzumdwuMfbQiICcuf/3oL2/dV8vMxPe1soIXO65vGgao6viy05iFzatwuBKpaB/yMxg/wNcCrqrpKRO4XkfNdm90iIqtEZBlwC3Cta989wAM0FpOFwP2uZQb43YRcROA3M9p234L9FbX87eNCzuyRzIjuSU7H8TsjuifRLiKED1YWOx3F+KkQT7yJqs4EZh617L4mz+8B7mlm32eBZz2Ro61Jj4vkjtE9ePC9NXywsphxbfR2yr9/Ukh5VS13j+vldBS/FBYSxNm9OvDRmlLq6hsICbZ+oqZl7DfGx107PIuctFh++84qDlS1vb4F2/dV8txX33DRwAx6p8U6HcdvjclNZc+hGgq2BNadZsYzrBD4uJDgIP5wUV9KD1Tz51nrnY7jcX+etQ6AO/N7OJzEv53ZI5mwkCBmrSpxOorxQ1YI/MCAzDiuGtqZaV9/06b6FqzasZ+3lmznhyOy6BgX6XQcvxYdHsLI7kl8uKq4TV9PMq3DCoGf+MXYnqTFRnDXa8vaxCBjqsrvZqwmPiqM/z2zu9Nx2oT83BS276tk1Y5yp6MYP2OFwE+0iwjljxf3Y1PZoSPNKf5sxrIdLPhmDz8f05P2UaFOx2kTzumdQpA0TuhjTEtYIfAjI7OTuXJoJ6Z+sZmCb/z3LttD1XU8NHMtfdJjuTQv88Q7mJOSGBNOXlYCs1bZbaSmZawQ+Jl7zu1Nelwkd722jMoa/2wimjy3kOLyKn53fh8bZtrD8nNSWFt8wCY4Mi1ihcDPxISH8MjF/fhmdwWPfOh/I5Ru3nWIqZ9v5qJB6QzuHLAjjreaMbmpAHb3kGkRKwR+aHi3JCYN68xzX37D5xv8ZyTWhgbl7jeWEx4axN1jrfNYa8hMiCInLZYPrXnItIAVAj9197jeZHeI4fZXllF2oPrEO/iAlxcWMX/zHu49tzcdYiOcjtNm5eemsGjrXr/5vTDOs0LgpyLDgnnyikEcqKrlzteW0dDg2/eOF++v4qGZaxjWNZHLTrMLxK1pTG4qqjDb7h4yJ8kKgR/rmdqOX4/P4bP1ZUz9YpPTcZqlqvx6+kpq6ht46KK+NrpoK+uV2o5OCVHMWm3NQ+bkWCHwc1cO7cS4Pqk88sE6n+11/GpBEbNXl3Bnfg+ykqKdjtPmiQj5OSl8Vbi7TY5PZTzPCoGfExEevqgfaXER/PSFRZSWVzkd6Vs2lR3ktzNWM7xbItd9r6vTcQLGmD6p1NQ3MHed/9xMYJxjhaANaB8Vyj+vyaO8so4bX1jkMzNV1dY3cNsrSwkLCeIvlw6wyei9aFCneBKjw+w6gTkpVgjaiF6psfz50v4s3rqP30z3jYlsfv/eGpZv288f/6cvqe3tLiFvCg4Szumdwty1pT7zxcD4LisEbci5fdP42VndeXlhEVM+c/bi8RuLtvGvr77hx9/rwtg+bXNCHV+Xn5vCweo65m3y3+FIjHdYIWhj7hjdg/P7d+Sh99fy+qJtjmRYsW0/v3prBad3TeAem3XMMSO6JxEVFmxjD5kTskLQxgQFCX+6pD/f657EL99Yzpw13m0j3rL7ED/810KSYsKZfMUgmzbRQRGhwZzZI5nZq0t8vp+JcZb9L22DwkKCeOrqweR2jOWnLyzmIy9dMCw7UM01zy6grqGBaT8aQmJMuFeOa5qXn5tC6YFqlm3b53QU48M8UghEZKyIrBORQhG5+xjr7xCR1SKyXETmiEjnJuvqRWSp6zHDE3lM4+B0//7REHqntePGFxbx/oqdrXq80gNVXDV1PqXl1Tx37Wl07xDTqsczJ+fsnikEB4nNUWCOy+1CICLBwGRgHJADXC4iOUdttgTIU9V+wOvAI03WVarqANfjfHfzmP+Kiwrj+euG0j8zjpv+s5hnv9jcKncTbd9XyaVPfU3R3gqemZTHwE42qqivaB8VyuldbY4Cc3yeOCMYAhSq6iZVrQFeBiY03UBV56pqhevlPCDDA8c1JyE2IpR//2gI5/RO4f53V/Ort1Z49HbCRVv2cMHkL9l9qIbnfzyU4d2TPPbexjNG905hY9khNpYddDqK8VGeKATpQFGT19tcy5rzY+D9Jq8jRKRAROaJyAXN7SQi17u2Kygrs96SLREdHsJTVw3mprO68dKCIiY8+SWr3ZzXtqFBee7LzUycMo/osGDe+Olwm1/AR412zVFgnctMc7x6sVhErgLygEebLO6sqnnAFcDjItLtWPuq6hRVzVPVvOTkZC+kbVuCgoSfj+nFs9fmsetgDRMmf8FDM9ewv7LlY9GsLS5n4j/n8bt3VjMyO5npN32PHintWiG18YT0uEj6pMda85BpVogH3mM70HRc4QzXsm8RkXOAe4EzVfXIQOmqut31c5OIfAIMBDZ6IJc5hrN7pTDr9nj+MHMNUz7fxCsFRVw+pBNXDOlEZkJUs/upKou27GXa11t4d/kO2oWH8OjF/bh4cIaNJuoH8nNSeeyj9ZSWV9lcEOY7xN2LhyISAqwHRtFYABYCV6jqqibbDKTxIvFYVd3QZHk8UKGq1SKSBHwNTFDV1cc7Zl5enhYUFLiV28CqHfv560cb+GhNCQ0KvdNiGdolgW4dYoiLDEWB0vIq1hUf4KuNu9m+r5LosGCuGZ7FDWd0JS4qzOk/gjlJa4vLGfv45/zhwr5cMbST03GMQ0RkkasF5lvcPiNQ1ToR+RnwIRAMPKuqq0TkfqBAVWfQ2BQUA7zm+va41XWHUG/gaRFpoLGZ6uETFQHjObkd2zPlmjy276tk+tLtfLqujFcWFlFZ++2LyXFRoQztksCt52RzXt80osM9cSJpvKlnyn/nKLBCYI7m9hmBE+yMoPWoKiXl1RysrkUVOrSLIDYyxJp/2oAH313Nv7/ewuL7RhNjxTwgNXdGYD2LzbeICKntI+jeoR3ZKe1oHxVqRaCNyM9tnKPgU5ujwBzFCoExAWJw53gSosNsCkvzHVYIjAkQwUHCqF4d+HhtKTV1DU7HMT7ECoExASQ/N5UDVXXM37zb6SjGh1ghMCaAjMxOIjI0mFmrrJex+S8rBMYEkIjQYM7okWRzFJhvsUJgTIDJz0mluLyKFdv3Ox3F+AgrBMYEmLN7dSA4SGwQOnOEFQJjAkx8dBhDshLsNlJzhBUCYwJQfm4K60sOsnnXIaejGB9ghcCYADQ6JwWA2XZWYLBCYExAyoiPIict1m4jNYAVAmMCVn5uCou27qXsQPWJNzZtmhUCYwJUfk4qqjBnjZ0VBDorBMYEqN5p7ciIj2SW3UYa8KwQGBOgRIT8nFS+KNzFoeo6p+MYB1khMCaA5eemUFPXwGfrbY6CQGaFwJgAltc5nvioUGseCnBWCIwJYCHBQZzdK4U5a0qorbc5CgKVFQJjAlx+bgrlVXUs2LzH6SjGIR4pBCIyVkTWiUihiNx9jPXhIvKKa/18Eclqsu4e1/J1IjLGE3mMMSfvjOxkIkKDmLXKehkHKrcLgYgEA5OBcUAOcLmI5By12Y+BvaraHXgM+KNr3xxgIpALjAX+7no/Y4yXRIYFMzI7mdmrS1C1OQoCkSfOCIYAhaq6SVVrgJeBCUdtMwGY5nr+OjBKRMS1/GVVrVbVzUCh6/2MMV6Un5PCjv1VrNpR7nQU4wBPFIJ0oKjJ622uZcfcRlXrgP1A4knuC4CIXC8iBSJSUFZmt7oZ40mjeqcQJFjzUIDym4vFqjpFVfNUNS85OdnpOMa0KQnRYZyWlWC3kQYoTxSC7UBmk9cZrmXH3EZEQoD2wO6T3NcY4wX5uamsLT7Alt02R0Gg8UQhWAhki0gXEQmj8eLvjKO2mQFMcj2/GPhYG69KzQAmuu4q6gJkAws8kMkY00L5R+YosLOCQON2IXC1+f8M+BBYA7yqqqtE5H4ROd+12TNAoogUAncAd7v2XQW8CqwGPgBuUtV6dzMZY1ouMyGKXqntbI6CABTiiTdR1ZnAzKOW3dfkeRVwSTP7/h74vSdyGGPck5+bypMfb2D3wWoSY8KdjmO8xG8uFhtjWl9+TgoNCnPWljodxXiRFQJjzBG5HWNJj4u05qEAY4XAGHOEiDA6J4XPN5RRUWNzFAQKKwTGmG/Jz02huq6Bz9bvcjqK8RIrBMaYbxmSlUBcVCgfrNzpdBTjJVYIjDHfEhIcxNjcVGavLqGq1u7mDgRWCIwx3zG+X0cO1dTzyTq7eygQWCEwxnzH6V0TSIwO453l1jwUCKwQGGO+IyQ4iLF9Uvl4TandPRQArBAYY45pfL+OVNbW87F1LmvzrBAYY45pSJcEktuF8+4yax5q66wQGGOOKThIOK9vGnPXlXKw2pqH2jIrBMaYZo3vl0Z1XQMf2dDUbZoVAmNMswZ1iic1NoJ37e6hNs0KgTGmWUFBwnn90vhsfRn7K2udjmNaiRUCY8xxje+XRk19g81c1oZZITDGHNeAzDgyEyKZvtSmE2+rrBAYY45LRLhwQDpfFu6ipLzK6TimFVghMMac0AUD02lQmLF0h9NRTCuwQmCMOaGuyTH0z4zjzSXWPNQWuVUIRCRBRGaLyAbXz/hjbDNARL4WkVUislxELmuy7l8isllElroeA9zJY4xpPRcNTGfNznLWFpc7HcV4mLtnBHcDc1Q1G5jjen20CuAaVc0FxgKPi0hck/U/V9UBrsdSN/MYY1rJD/p3JCRIeGuxnRW0Ne4WggnANNfzacAFR2+gqutVdYPr+Q6gFEh287jGGC9LiA7j+z2TeXvpduob1Ok4xoPcLQQpqnq4y2ExkHK8jUVkCBAGbGyy+PeuJqPHRCT8OPteLyIFIlJQVlbmZmxjzKm4cGAGJeXVfL1xt9NRjAedsBCIyEcisvIYjwlNt1NVBZr9miAiacDzwA9VtcG1+B6gF3AakAD8srn9VXWKquapal5ysp1QGOOEUb070C48hDeXbHM6ivGgkBNtoKrnNLdOREpEJE1Vd7o+6I85cLmIxALvAfeq6rwm7334bKJaRJ4D7mpRemOMV0WEBnNu3zTeWb6DBzfB5QoAAA6sSURBVCbUER1+wo8Q4wfcbRqaAUxyPZ8ETD96AxEJA94C/q2qrx+1Ls31U2i8vrDSzTzGmFZ2SV4GFTX1vGcD0bUZ7haCh4HRIrIBOMf1GhHJE5Gprm0uBc4Arj3GbaIvisgKYAWQBDzoZh5jTCsb3Dme7h1ieHnhVqejGA9x67xOVXcDo46xvAC4zvX8BeCFZvY/253jG2O8T0SYeFomD763hvUlB+iR0s7pSMZN1rPYGNNiFw5MJzRYeGVhkdNRjAdYITDGtFhiTDj5Oam8uXgb1XX1TscxbrJCYIw5JROHZLK3opZZq2yeAn9nhcAYc0pGdEsiPS7SmofaACsExphTEhQkXHZaJl8U7mLL7kNOxzFusEJgjDlll52WSUiQ8PzXW5yOYtxghcAYc8pSYiMY2yeVVwuKqKipczqOOUVWCIwxbrl2eBblVXW8vcRmL/NXVgiMMW4Z3Dme3I6xTPvqGxrHnjT+xgqBMcYtIsKk4VmsKznAvE17nI5jToEVAmOM287v35H4qFCmffWN01HMKbBCYIxxW0RoMBOHdGLW6mKK9lQ4Hce0kBUCY4xHTBqWRXCQMPXzTU5HMS1khcAY4xGp7SO4cGA6rxQUsftgtdNxTAtYITDGeMz1Z3Sjuq7BrhX4GSsExhiP6d4hhvycFKZ9vYVD1dbBzF9YITDGeNSNZ3Zjf2UtLy2wGcz8hRUCY4xHDewUz+ldE/jn55uoqrW5CvyBFQJjjMfdOqoHJeXVvDjfzgr8gRUCY4zHDeuWyIjuifzjk0IbjM4PuFUIRCRBRGaLyAbXz/hmtqsXkaWux4wmy7uIyHwRKRSRV0QkzJ08xhjfccfonuw6WMO0r2yIal/n7hnB3cAcVc0G5rheH0ulqg5wPc5vsvyPwGOq2h3YC/zYzTzGGB8xuHM8Z/VM5unPNnKgqtbpOOY43C0EE4BprufTgAtOdkcREeBs4PVT2d8Y4/vuzO/Jvopanvp0o9NRzHG4WwhSVHWn63kxkNLMdhEiUiAi80Tk8Id9IrBPVQ83IG4D0ps7kIhc73qPgrKyMjdjG2O8oU96ey4Y0JF/fr7ZxiDyYScsBCLykYisPMZjQtPttHEg8uYGI++sqnnAFcDjItKtpUFVdYqq5qlqXnJyckt3N8Y45JfjehEk8PAHa52OYppxwkKgqueoap9jPKYDJSKSBuD6WdrMe2x3/dwEfAIMBHYDcSIS4tosA9ju9p/IGONT0tpHcuOZ3Xhv+U4WbLb5CnyRu01DM4BJrueTgOlHbyAi8SIS7nqeBIwAVrvOIOYCFx9vf2OM/7vhjG6ktY/gtzNWUVff4HQccxR3C8HDwGgR2QCc43qNiOSJyFTXNr2BAhFZRuMH/8Oqutq17pfAHSJSSOM1g2fczGOM8UGRYcH8enwOq3eW88wXm52OY44i/jjHaF5enhYUFDgdwxjTAqrK9c8v4rP1ZXx42xlkJUU7HSngiMgi1/Xab7GexcYYrxARHpjQh7DgIH711gqb6N6HWCEwxnhNavsIfjmuF19t3M2/bM4Cn2GFwBjjVVcO7cTZvTrw0My1rN5R7nQcgxUCY4yXiQiPXtyP9lGh3PzSYiprbKhqp1khMMZ4XWJMOI9fNoBNuw7xizeW2/UCh4WceBNjjPG8Ed2T+MWYXvzxg7X06BDDzaOynY50ShoalM27D7Gh5CCFpQfYtreSvRU17KuoRYGQICE6PIS09hF0jIskt2MsfdPbExflO4MtWyEwxjjmxjO7sqHkAH+evZ4uydGM79fR6UgnZevuCuasLWHept0s2LyHvRX/HV01KSachOhQ4iLDEIGaugZ2HTzE/E27Ka/679wMOWmxnJOTwrg+qfROi3Xij3GEFQJjjGNEhD9c1JeivRXc9vJSosKCObtXc2NXOmtT2UHeW76T91cWs3pn40XuzIRIRvVOYUhWAr3S2tEtOYbo8OY/VvdX1LJyx36WFu3jk3WlPPnxBp6Ys4EBmXFcMbQTEwZ0JDwk2Ft/pCOsQ5kxxnHlVbVcNXU+a4sPMPWaPM7o4RsDS1bV1vPhqmL+M38r813jJA3uHM/Y3FTG5KbSKTHKrffffbCa6Ut38J8FWyksPUh6XCQ3ndWdiwdnEBbi+Uu4zXUos0JgjPEJew/VcMXU+RSWHuDRi/tzwcBmR6VvdRtKDvDSgiLeXLKNfRW1dE6M4rLTMrloYAap7SM8fjxV5fMNu3jso/Us2bqPrsnRPDChDyO6J3n0OFYIjDE+r7yqlhv+vYivN+3m1lHZ3DIqm+Ag8cqxq2rrmbliJy8t2MrCb/YSGizk56ZyxZBODOuaSJAXcqgqH68t5f53V7NldwXj+6Vx/4Q+JER75sKyFQJjjF+orqvnnjdX8Obi7Qzvlshjlw0gJdbz38IPW1tczssLinhz8TbKq+rokhTN5UMy+Z9BGSTGhLfacY+nqraepz7dyN/nbiQuKpQ/XdLfI81lVgiMMX5DVXlt0Tbum76SkKAgbjsnm0nDswgN9ky7eXlVLR+sKOblhVtZvHUfYcFBjO2TysQhmQzrmkjjTLrOW72jnFtfXsKG0oNc970u/HJcL7f+DqwQGGP8zje7DvHbd1bxyboyMuIj+cnIrlw8OOO4d+Y0Z39FLZ+sL+WdZTv5bH0ZNfUNdEuO5vIhnbhoUIbHml88raq2nt+/t4bn521haJcEnrxiEMntTu1MxQqBMcYvqSpz15Uyee5GFm3ZS0RoEKN6pTAyO4lBnePJSoz+zh02dfUNFO2tZM3OclZu38+XG3ezYts+GhRSYsMZ368j4/ulMSAzzme+/Z/IW0u28cf31/HqDcNO+W4lKwTGGL+3aMtepi/dzswVxew6WH1keVJMGJFhwQSJcLCqjj0VNRz+aAsOEgZkxjGiexJnZCcxqFO8Vy78toaq2noiQk+9n4EVAmNMm6GqbN51iKVF+9i6p4Li/VXU1DVQr0pMeAhJMeGkx0XSOy2W7JQYtz4825LmCoH1LDbG+B0RoWtyDF2TY5yO0ibY6KPGGBPg3CoEIpIgIrNFZIPrZ/wxtjlLRJY2eVSJyAWudf8Skc1N1g1wJ48xxpiWc/eM4G5gjqpmA3Ncr79FVeeq6gBVHQCcDVQAs5ps8vPD61V1qZt5jDHGtJC7hWACMM31fBpwwQm2vxh4X1Ur3DyuMcYYD3G3EKSo6k7X82LgROPHTgReOmrZ70VkuYg8JiLO9Oc2xpgAdsK7hkTkIyD1GKvubfpCVVVEmr0XVUTSgL7Ah00W30NjAQkDpgC/BO5vZv/rgesBOnXqdKLYxhhjTtIJC4GqntPcOhEpEZE0Vd3p+qAvPc5bXQq8papHpvJpcjZRLSLPAXcdJ8cUGosFeXl5/tf5wRhjfJS7TUMzgEmu55OA6cfZ9nKOahZyFQ+ksY/3BcBKN/MYY4xpIbd6FotIIvAq0AnYAlyqqntEJA+4UVWvc22XBXwJZKpqQ5P9PwaSAQGWuvY5eBLHLXMd71QkAbtOcd/WZLlaxnK1jOVqmbaaq7Oqfmc8a78cYsIdIlJwrC7WTrNcLWO5WsZytUyg5bKexcYYE+CsEBhjTIALxEIwxekAzbBcLWO5WsZytUxA5Qq4awTGGGO+LRDPCIwxxjRhhcAYYwJcQBYCERkgIvNcQ18XiMgQpzMdJiI3i8haEVklIo84nacpEblTRFREkpzOAiAij7r+rpaLyFsiEudwnrEisk5ECkXkOyPxOkFEMkVkroisdv1O3ep0pqZEJFhElojIu05nOUxE4kTkddfv1hoRGeZ0JgARud31b7hSRF4SkQhPvXdAFgLgEeB3rqGx73O9dpyInEXjiK79VTUX+JPDkY4QkUwgH9jqdJYmZgN9VLUfsJ7GsascISLBwGRgHJADXC4iOU7laaIOuFNVc4DTgZt8JNdhtwJrnA5xlL8CH6hqL6A/PpBPRNKBW4A8Ve0DBNM4iKdHBGohUCDW9bw9sMPBLE39FHhYVasBVPV4Yzd522PAL2j8u/MJqjpLVetcL+cBGQ7GGQIUquomVa0BXqaxqDtKVXeq6mLX8wM0fqilO5uqkYhkAOcBU53OcpiItAfOAJ4BUNUaVd3nbKojQoBIEQkBovDg51agFoLbgEdFpIjGb92OfZM8Sg9gpIjMF5FPReQ0pwMBiMgEYLuqLnM6y3H8CHjfweOnA0VNXm/DRz5wD3MN9TIQmO9skiMep/HLRcOJNvSiLkAZ8JyryWqqiEQ7HUpVt9P4WbUV2AnsV9VZx9/r5LXZyetPMHz2KOB2VX1DRC6lsfo3O8qqF3OFAAk0nsKfBrwqIl3VC/f4niDXr2hsFvK64+VS1emube6lsQnkRW9m8yciEgO8AdymquU+kGc8UKqqi0Tk+07naSIEGATcrKrzReSvNM68+GsnQ7mmAZ5AY6HaB7wmIlep6gueeP82WwhOMHz2v2lsmwR4DS+emp4g10+BN10f/AtEpIHGQabKnMolIn1p/OVb1jhILBnAYhEZoqrFTuVqku9aYDwwyhsF8zi2A5lNXme4ljlOREJpLAIvquqbTudxGQGcLyLnAhFArIi8oKpXOZxrG7BNVQ+fNb3OMabgdcA5wGZVLQMQkTeB4YBHCkGgNg3tAM50PT8b2OBglqbeBs4CEJEeNE7Y4+gIiKq6QlU7qGqWqmbR+B9lkDeKwImIyFgamxbO94HpTxcC2SLSRUTCaLyQN8PhTIeHeH8GWKOqf3E6z2Gqeo+qZrh+pyYCH/tAEcD1e10kIj1di0YBqx2MdNhW4HQRiXL9m47Cgxex2+wZwQn8BPir66JLFa6Zz3zAs8CzIrISqAEmOfwt19c9CYQDs11nK/NU9UYngqhqnYj8jMYZ+IKBZ1V1lRNZjjICuBpYISJLXct+paozHczk624GXnQV9E3ADx3Og6uZ6nVgMY3NoEvw4HATNsSEMcYEuEBtGjLGGONihcAYYwKcFQJjjAlwVgiMMSbAWSEwxpgAZ4XAGGMCnBUCY4wJcP8Pz0kiaWm1yA4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX3sQ0EsKFn0"
      },
      "source": [
        "real_data = torch.utils.data.DataLoader(train_data,\n",
        "                                       batch_size=batch_size,\n",
        "                                       shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTn5Fkv3KFkQ"
      },
      "source": [
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "adversarial_loss = torch.nn.BCELoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpBxTJA6-g5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc0fe44-43bc-40b8-d94c-a70528708587"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "d_loss_history = []\n",
        "g_loss_history = []\n",
        "\n",
        "red_patch = mpatches.Patch(color='red', label='D loss')\n",
        "green_patch = mpatches.Patch(color='green', label='G loss')\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (train_values) in enumerate(real_data):\n",
        "\n",
        "        batch_size = train_values.shape[0]\n",
        "##################### Лейблы для данных: 1 - настоящие, 0 - сгенерированные ########\n",
        "        valid = Variable(torch.FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
        "        fake = Variable(torch.FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
        "\n",
        "        real_values = Variable(train_values.type(torch.FloatTensor)).to(device)\n",
        "\n",
        "\n",
        "######################  Тренировка генератора    ##########################\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "    \n",
        "        #генерация шума\n",
        "        z = Variable(torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))).to(device)\n",
        "\n",
        "        gen_values = generator(z)\n",
        "\n",
        "        validity = discriminator(gen_values)\n",
        "        g_loss = adversarial_loss(validity, valid)\n",
        "        \n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "######################  Тренировка дискриминатора    ##########################\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_pred = discriminator(real_values)\n",
        "        d_real_loss = adversarial_loss(real_pred, valid)\n",
        "\n",
        "        fake_pred = discriminator(gen_values.detach())\n",
        "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
        "\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        \n",
        "######## Отображение процесса обучения и вывод функций потерь ############\n",
        "        batches_done = epoch * len(real_data) + i\n",
        "    \n",
        "        if batches_done % sample_interval == 0:\n",
        "            # plt.clf()\n",
        "            \n",
        "            display.clear_output(wait=False)\n",
        "            # sample_image(gen_values)\n",
        "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"% (epoch, n_epochs, i, len(real_data), d_loss.item(), g_loss.item()) ) \n",
        "\n",
        "            \n",
        "            # display.display(plt.gcf())\n",
        "\n",
        "            # d_loss_history.append(d_loss)\n",
        "            # g_loss_history.append(g_loss)\n",
        "\n",
        "            # plt.plot(np.log(np.array(d_loss_history)), label='D loss', color = 'red')\n",
        "            # plt.plot(np.log(np.array(g_loss_history)), label='G loss', color = 'green')\n",
        "            # plt.legend(handles=[red_patch, green_patch])\n",
        "            # plt.show()\n",
        "\n",
        "            "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 9/10] [Batch 700/800] [D loss: 0.679905] [G loss: 0.706387]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClQ_JVxE-g1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "3313349b-37e9-4d79-c9be-1a172991bcbe"
      },
      "source": [
        "z = Variable(torch.FloatTensor(np.random.normal(0, 1, (300,2)))).to(device)\n",
        "\n",
        "plt.plot(x, y, 'r', label='real data', color = 'red', linewidth=5)\n",
        "\n",
        "pred = generator(z).detach().numpy()\n",
        "plt.plot(pred[:,0], pred[:,1], '.', label='generate', color = 'indigo')\n",
        "plt.legend()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1f5eda2da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVja5Xe7-gyI"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKgClM6RXtXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tRpb8HYXtYD"
      },
      "source": [
        "2. Изменить используемые модели генератора и дискриминатора, с помощью сверточных слоев. Идея: https://arxiv.org/abs/1511.06434 Датасет можно использовать так же MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHuq4sl2H2R-"
      },
      "source": [
        "Ознакомившись с материалом по указанной в условии ссылке, нашел решение https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py\n",
        "Попробую его доработать "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTAj2B8XXtYH"
      },
      "source": [
        "######## функция для вывода и сохранения изображения ########\n",
        "def sample_image(static_sample, save_img = False):\n",
        "    npimg = make_grid(static_sample.data[:25]).cpu().numpy()\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    ax.imshow(np.transpose(npimg, (1,2,0)), interpolation=\"nearest\")\n",
        "    if save_img:\n",
        "        save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4b7_NelGqNI"
      },
      "source": [
        "import argparse"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks7Ei4lmXtYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ebdb57-5488-4f6b-c8e0-833efe331a8b"
      },
      "source": [
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=1, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
        "opt, unknown = parser.parse_known_args() # opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=28, latent_dim=100, lr=0.0002, n_cpu=8, n_epochs=1, sample_interval=400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQrPli2rXtY9"
      },
      "source": [
        "# dataiter = iter(real_data)\n",
        "# images, labels = dataiter.next()\n",
        "# images = images.numpy()\n",
        "\n",
        "# img = np.squeeze(images[0])\n",
        "# img1 = np.squeeze(images[1])\n",
        "# img2 = np.squeeze(images[2])\n",
        "\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12,4))\n",
        "# ax1.imshow(img, cmap='gray')\n",
        "# ax2.imshow(img1, cmap='gray')\n",
        "# ax3.imshow(img2, cmap='gray')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFjXKCwatVUk"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_psjKBq_tVDB"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XKkAlJXXtYt"
      },
      "source": [
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EswV7gyO8JJe"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us3OUpCKtwMk",
        "outputId": "d50cad13-bcba-4d7c-bf88-daebcb7f6748"
      },
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/1] [Batch 0/938] [D loss: 0.680925] [G loss: 0.680354]\n",
            "[Epoch 0/1] [Batch 1/938] [D loss: 0.593739] [G loss: 0.677114]\n",
            "[Epoch 0/1] [Batch 2/938] [D loss: 0.531432] [G loss: 0.673928]\n",
            "[Epoch 0/1] [Batch 3/938] [D loss: 0.478618] [G loss: 0.670205]\n",
            "[Epoch 0/1] [Batch 4/938] [D loss: 0.442306] [G loss: 0.665830]\n",
            "[Epoch 0/1] [Batch 5/938] [D loss: 0.420352] [G loss: 0.660617]\n",
            "[Epoch 0/1] [Batch 6/938] [D loss: 0.404307] [G loss: 0.653989]\n",
            "[Epoch 0/1] [Batch 7/938] [D loss: 0.396396] [G loss: 0.647760]\n",
            "[Epoch 0/1] [Batch 8/938] [D loss: 0.394266] [G loss: 0.637603]\n",
            "[Epoch 0/1] [Batch 9/938] [D loss: 0.396291] [G loss: 0.627787]\n",
            "[Epoch 0/1] [Batch 10/938] [D loss: 0.399185] [G loss: 0.617970]\n",
            "[Epoch 0/1] [Batch 11/938] [D loss: 0.405937] [G loss: 0.605582]\n",
            "[Epoch 0/1] [Batch 12/938] [D loss: 0.410446] [G loss: 0.596849]\n",
            "[Epoch 0/1] [Batch 13/938] [D loss: 0.413893] [G loss: 0.590670]\n",
            "[Epoch 0/1] [Batch 14/938] [D loss: 0.420805] [G loss: 0.580324]\n",
            "[Epoch 0/1] [Batch 15/938] [D loss: 0.421446] [G loss: 0.581258]\n",
            "[Epoch 0/1] [Batch 16/938] [D loss: 0.426011] [G loss: 0.577901]\n",
            "[Epoch 0/1] [Batch 17/938] [D loss: 0.428117] [G loss: 0.578419]\n",
            "[Epoch 0/1] [Batch 18/938] [D loss: 0.428310] [G loss: 0.586992]\n",
            "[Epoch 0/1] [Batch 19/938] [D loss: 0.428373] [G loss: 0.594044]\n",
            "[Epoch 0/1] [Batch 20/938] [D loss: 0.427622] [G loss: 0.603140]\n",
            "[Epoch 0/1] [Batch 21/938] [D loss: 0.425340] [G loss: 0.614300]\n",
            "[Epoch 0/1] [Batch 22/938] [D loss: 0.425059] [G loss: 0.616104]\n",
            "[Epoch 0/1] [Batch 23/938] [D loss: 0.426230] [G loss: 0.619728]\n",
            "[Epoch 0/1] [Batch 24/938] [D loss: 0.432903] [G loss: 0.610980]\n",
            "[Epoch 0/1] [Batch 25/938] [D loss: 0.438958] [G loss: 0.628549]\n",
            "[Epoch 0/1] [Batch 26/938] [D loss: 0.441492] [G loss: 0.610974]\n",
            "[Epoch 0/1] [Batch 27/938] [D loss: 0.457846] [G loss: 0.592583]\n",
            "[Epoch 0/1] [Batch 28/938] [D loss: 0.460940] [G loss: 0.623377]\n",
            "[Epoch 0/1] [Batch 29/938] [D loss: 0.476840] [G loss: 0.561622]\n",
            "[Epoch 0/1] [Batch 30/938] [D loss: 0.484397] [G loss: 0.575163]\n",
            "[Epoch 0/1] [Batch 31/938] [D loss: 0.494191] [G loss: 0.583858]\n",
            "[Epoch 0/1] [Batch 32/938] [D loss: 0.498273] [G loss: 0.578015]\n",
            "[Epoch 0/1] [Batch 33/938] [D loss: 0.514135] [G loss: 0.545133]\n",
            "[Epoch 0/1] [Batch 34/938] [D loss: 0.506209] [G loss: 0.599275]\n",
            "[Epoch 0/1] [Batch 35/938] [D loss: 0.511062] [G loss: 0.566659]\n",
            "[Epoch 0/1] [Batch 36/938] [D loss: 0.505263] [G loss: 0.582466]\n",
            "[Epoch 0/1] [Batch 37/938] [D loss: 0.507479] [G loss: 0.597108]\n",
            "[Epoch 0/1] [Batch 38/938] [D loss: 0.503831] [G loss: 0.623780]\n",
            "[Epoch 0/1] [Batch 39/938] [D loss: 0.509890] [G loss: 0.582665]\n",
            "[Epoch 0/1] [Batch 40/938] [D loss: 0.507981] [G loss: 0.627797]\n",
            "[Epoch 0/1] [Batch 41/938] [D loss: 0.483246] [G loss: 0.647570]\n",
            "[Epoch 0/1] [Batch 42/938] [D loss: 0.482642] [G loss: 0.640099]\n",
            "[Epoch 0/1] [Batch 43/938] [D loss: 0.498858] [G loss: 0.696899]\n",
            "[Epoch 0/1] [Batch 44/938] [D loss: 0.543514] [G loss: 0.541337]\n",
            "[Epoch 0/1] [Batch 45/938] [D loss: 0.480453] [G loss: 0.654620]\n",
            "[Epoch 0/1] [Batch 46/938] [D loss: 0.514278] [G loss: 0.728036]\n",
            "[Epoch 0/1] [Batch 47/938] [D loss: 0.584207] [G loss: 0.489657]\n",
            "[Epoch 0/1] [Batch 48/938] [D loss: 0.559254] [G loss: 0.763483]\n",
            "[Epoch 0/1] [Batch 49/938] [D loss: 0.612152] [G loss: 0.473196]\n",
            "[Epoch 0/1] [Batch 50/938] [D loss: 0.598195] [G loss: 0.702713]\n",
            "[Epoch 0/1] [Batch 51/938] [D loss: 0.587066] [G loss: 0.501468]\n",
            "[Epoch 0/1] [Batch 52/938] [D loss: 0.576463] [G loss: 0.718808]\n",
            "[Epoch 0/1] [Batch 53/938] [D loss: 0.560073] [G loss: 0.510896]\n",
            "[Epoch 0/1] [Batch 54/938] [D loss: 0.556193] [G loss: 0.871163]\n",
            "[Epoch 0/1] [Batch 55/938] [D loss: 0.589522] [G loss: 0.494773]\n",
            "[Epoch 0/1] [Batch 56/938] [D loss: 0.535202] [G loss: 0.653948]\n",
            "[Epoch 0/1] [Batch 57/938] [D loss: 0.504973] [G loss: 0.826820]\n",
            "[Epoch 0/1] [Batch 58/938] [D loss: 0.522202] [G loss: 0.609878]\n",
            "[Epoch 0/1] [Batch 59/938] [D loss: 0.484708] [G loss: 0.791791]\n",
            "[Epoch 0/1] [Batch 60/938] [D loss: 0.459448] [G loss: 0.727154]\n",
            "[Epoch 0/1] [Batch 61/938] [D loss: 0.441319] [G loss: 0.904325]\n",
            "[Epoch 0/1] [Batch 62/938] [D loss: 0.483598] [G loss: 0.636884]\n",
            "[Epoch 0/1] [Batch 63/938] [D loss: 0.457638] [G loss: 0.961121]\n",
            "[Epoch 0/1] [Batch 64/938] [D loss: 0.490765] [G loss: 0.615084]\n",
            "[Epoch 0/1] [Batch 65/938] [D loss: 0.450186] [G loss: 0.941473]\n",
            "[Epoch 0/1] [Batch 66/938] [D loss: 0.464390] [G loss: 0.726246]\n",
            "[Epoch 0/1] [Batch 67/938] [D loss: 0.444032] [G loss: 0.823121]\n",
            "[Epoch 0/1] [Batch 68/938] [D loss: 0.455990] [G loss: 0.858102]\n",
            "[Epoch 0/1] [Batch 69/938] [D loss: 0.532481] [G loss: 0.689503]\n",
            "[Epoch 0/1] [Batch 70/938] [D loss: 0.485475] [G loss: 0.808138]\n",
            "[Epoch 0/1] [Batch 71/938] [D loss: 0.491368] [G loss: 0.710020]\n",
            "[Epoch 0/1] [Batch 72/938] [D loss: 0.469626] [G loss: 0.830861]\n",
            "[Epoch 0/1] [Batch 73/938] [D loss: 0.515303] [G loss: 0.672707]\n",
            "[Epoch 0/1] [Batch 74/938] [D loss: 0.472778] [G loss: 0.824385]\n",
            "[Epoch 0/1] [Batch 75/938] [D loss: 0.499296] [G loss: 0.722003]\n",
            "[Epoch 0/1] [Batch 76/938] [D loss: 0.451540] [G loss: 0.723858]\n",
            "[Epoch 0/1] [Batch 77/938] [D loss: 0.449523] [G loss: 1.013874]\n",
            "[Epoch 0/1] [Batch 78/938] [D loss: 0.547300] [G loss: 0.527890]\n",
            "[Epoch 0/1] [Batch 79/938] [D loss: 0.489458] [G loss: 1.010239]\n",
            "[Epoch 0/1] [Batch 80/938] [D loss: 0.559403] [G loss: 0.507935]\n",
            "[Epoch 0/1] [Batch 81/938] [D loss: 0.491818] [G loss: 0.938580]\n",
            "[Epoch 0/1] [Batch 82/938] [D loss: 0.518449] [G loss: 0.560087]\n",
            "[Epoch 0/1] [Batch 83/938] [D loss: 0.453407] [G loss: 0.864088]\n",
            "[Epoch 0/1] [Batch 84/938] [D loss: 0.529839] [G loss: 0.643227]\n",
            "[Epoch 0/1] [Batch 85/938] [D loss: 0.499632] [G loss: 0.690638]\n",
            "[Epoch 0/1] [Batch 86/938] [D loss: 0.480315] [G loss: 0.772290]\n",
            "[Epoch 0/1] [Batch 87/938] [D loss: 0.512290] [G loss: 0.655919]\n",
            "[Epoch 0/1] [Batch 88/938] [D loss: 0.441343] [G loss: 0.794700]\n",
            "[Epoch 0/1] [Batch 89/938] [D loss: 0.499180] [G loss: 0.853712]\n",
            "[Epoch 0/1] [Batch 90/938] [D loss: 0.503495] [G loss: 0.609427]\n",
            "[Epoch 0/1] [Batch 91/938] [D loss: 0.497107] [G loss: 1.068428]\n",
            "[Epoch 0/1] [Batch 92/938] [D loss: 0.517036] [G loss: 0.582100]\n",
            "[Epoch 0/1] [Batch 93/938] [D loss: 0.422619] [G loss: 0.817461]\n",
            "[Epoch 0/1] [Batch 94/938] [D loss: 0.407869] [G loss: 1.000445]\n",
            "[Epoch 0/1] [Batch 95/938] [D loss: 0.411536] [G loss: 0.732683]\n",
            "[Epoch 0/1] [Batch 96/938] [D loss: 0.384692] [G loss: 0.956791]\n",
            "[Epoch 0/1] [Batch 97/938] [D loss: 0.370704] [G loss: 0.892212]\n",
            "[Epoch 0/1] [Batch 98/938] [D loss: 0.373228] [G loss: 0.860983]\n",
            "[Epoch 0/1] [Batch 99/938] [D loss: 0.376220] [G loss: 0.900993]\n",
            "[Epoch 0/1] [Batch 100/938] [D loss: 0.407063] [G loss: 0.896576]\n",
            "[Epoch 0/1] [Batch 101/938] [D loss: 0.419267] [G loss: 0.718388]\n",
            "[Epoch 0/1] [Batch 102/938] [D loss: 0.380243] [G loss: 0.984703]\n",
            "[Epoch 0/1] [Batch 103/938] [D loss: 0.374708] [G loss: 0.910357]\n",
            "[Epoch 0/1] [Batch 104/938] [D loss: 0.407582] [G loss: 0.878487]\n",
            "[Epoch 0/1] [Batch 105/938] [D loss: 0.420477] [G loss: 1.027089]\n",
            "[Epoch 0/1] [Batch 106/938] [D loss: 0.427749] [G loss: 0.817048]\n",
            "[Epoch 0/1] [Batch 107/938] [D loss: 0.432322] [G loss: 0.890580]\n",
            "[Epoch 0/1] [Batch 108/938] [D loss: 0.474518] [G loss: 0.810119]\n",
            "[Epoch 0/1] [Batch 109/938] [D loss: 0.520312] [G loss: 0.916276]\n",
            "[Epoch 0/1] [Batch 110/938] [D loss: 0.512127] [G loss: 0.645335]\n",
            "[Epoch 0/1] [Batch 111/938] [D loss: 0.499736] [G loss: 0.942658]\n",
            "[Epoch 0/1] [Batch 112/938] [D loss: 0.470034] [G loss: 0.840019]\n",
            "[Epoch 0/1] [Batch 113/938] [D loss: 0.496833] [G loss: 0.796122]\n",
            "[Epoch 0/1] [Batch 114/938] [D loss: 0.538619] [G loss: 0.807959]\n",
            "[Epoch 0/1] [Batch 115/938] [D loss: 0.547874] [G loss: 0.728104]\n",
            "[Epoch 0/1] [Batch 116/938] [D loss: 0.548493] [G loss: 0.875546]\n",
            "[Epoch 0/1] [Batch 117/938] [D loss: 0.546242] [G loss: 0.728969]\n",
            "[Epoch 0/1] [Batch 118/938] [D loss: 0.528787] [G loss: 0.844319]\n",
            "[Epoch 0/1] [Batch 119/938] [D loss: 0.511641] [G loss: 0.855018]\n",
            "[Epoch 0/1] [Batch 120/938] [D loss: 0.523823] [G loss: 0.866996]\n",
            "[Epoch 0/1] [Batch 121/938] [D loss: 0.481240] [G loss: 0.842201]\n",
            "[Epoch 0/1] [Batch 122/938] [D loss: 0.462083] [G loss: 1.049584]\n",
            "[Epoch 0/1] [Batch 123/938] [D loss: 0.451902] [G loss: 0.926152]\n",
            "[Epoch 0/1] [Batch 124/938] [D loss: 0.412736] [G loss: 1.027790]\n",
            "[Epoch 0/1] [Batch 125/938] [D loss: 0.421860] [G loss: 1.092151]\n",
            "[Epoch 0/1] [Batch 126/938] [D loss: 0.423242] [G loss: 0.886273]\n",
            "[Epoch 0/1] [Batch 127/938] [D loss: 0.455723] [G loss: 1.088290]\n",
            "[Epoch 0/1] [Batch 128/938] [D loss: 0.500537] [G loss: 0.719453]\n",
            "[Epoch 0/1] [Batch 129/938] [D loss: 0.522589] [G loss: 0.935919]\n",
            "[Epoch 0/1] [Batch 130/938] [D loss: 0.571123] [G loss: 0.694619]\n",
            "[Epoch 0/1] [Batch 131/938] [D loss: 0.617957] [G loss: 0.625493]\n",
            "[Epoch 0/1] [Batch 132/938] [D loss: 0.601915] [G loss: 0.692613]\n",
            "[Epoch 0/1] [Batch 133/938] [D loss: 0.587781] [G loss: 0.656799]\n",
            "[Epoch 0/1] [Batch 134/938] [D loss: 0.543005] [G loss: 0.862511]\n",
            "[Epoch 0/1] [Batch 135/938] [D loss: 0.492873] [G loss: 0.716880]\n",
            "[Epoch 0/1] [Batch 136/938] [D loss: 0.480780] [G loss: 1.031420]\n",
            "[Epoch 0/1] [Batch 137/938] [D loss: 0.463995] [G loss: 0.809124]\n",
            "[Epoch 0/1] [Batch 138/938] [D loss: 0.448457] [G loss: 0.904997]\n",
            "[Epoch 0/1] [Batch 139/938] [D loss: 0.450713] [G loss: 1.001636]\n",
            "[Epoch 0/1] [Batch 140/938] [D loss: 0.494952] [G loss: 0.712752]\n",
            "[Epoch 0/1] [Batch 141/938] [D loss: 0.534398] [G loss: 1.129805]\n",
            "[Epoch 0/1] [Batch 142/938] [D loss: 0.662599] [G loss: 0.447338]\n",
            "[Epoch 0/1] [Batch 143/938] [D loss: 0.575994] [G loss: 0.947250]\n",
            "[Epoch 0/1] [Batch 144/938] [D loss: 0.599690] [G loss: 0.652969]\n",
            "[Epoch 0/1] [Batch 145/938] [D loss: 0.649086] [G loss: 0.643935]\n",
            "[Epoch 0/1] [Batch 146/938] [D loss: 0.660510] [G loss: 0.700342]\n",
            "[Epoch 0/1] [Batch 147/938] [D loss: 0.650160] [G loss: 0.553049]\n",
            "[Epoch 0/1] [Batch 148/938] [D loss: 0.642510] [G loss: 0.732779]\n",
            "[Epoch 0/1] [Batch 149/938] [D loss: 0.652903] [G loss: 0.537846]\n",
            "[Epoch 0/1] [Batch 150/938] [D loss: 0.581343] [G loss: 0.726667]\n",
            "[Epoch 0/1] [Batch 151/938] [D loss: 0.556380] [G loss: 0.763646]\n",
            "[Epoch 0/1] [Batch 152/938] [D loss: 0.568992] [G loss: 0.623720]\n",
            "[Epoch 0/1] [Batch 153/938] [D loss: 0.534508] [G loss: 0.751624]\n",
            "[Epoch 0/1] [Batch 154/938] [D loss: 0.569560] [G loss: 0.687813]\n",
            "[Epoch 0/1] [Batch 155/938] [D loss: 0.593818] [G loss: 0.557876]\n",
            "[Epoch 0/1] [Batch 156/938] [D loss: 0.622324] [G loss: 0.834130]\n",
            "[Epoch 0/1] [Batch 157/938] [D loss: 0.660377] [G loss: 0.476638]\n",
            "[Epoch 0/1] [Batch 158/938] [D loss: 0.634744] [G loss: 0.692237]\n",
            "[Epoch 0/1] [Batch 159/938] [D loss: 0.624845] [G loss: 0.632492]\n",
            "[Epoch 0/1] [Batch 160/938] [D loss: 0.604636] [G loss: 0.604923]\n",
            "[Epoch 0/1] [Batch 161/938] [D loss: 0.612955] [G loss: 0.784952]\n",
            "[Epoch 0/1] [Batch 162/938] [D loss: 0.593975] [G loss: 0.544682]\n",
            "[Epoch 0/1] [Batch 163/938] [D loss: 0.537533] [G loss: 0.757974]\n",
            "[Epoch 0/1] [Batch 164/938] [D loss: 0.558567] [G loss: 0.816185]\n",
            "[Epoch 0/1] [Batch 165/938] [D loss: 0.606723] [G loss: 0.510462]\n",
            "[Epoch 0/1] [Batch 166/938] [D loss: 0.590921] [G loss: 0.934666]\n",
            "[Epoch 0/1] [Batch 167/938] [D loss: 0.627999] [G loss: 0.497146]\n",
            "[Epoch 0/1] [Batch 168/938] [D loss: 0.614867] [G loss: 0.714872]\n",
            "[Epoch 0/1] [Batch 169/938] [D loss: 0.598370] [G loss: 0.655941]\n",
            "[Epoch 0/1] [Batch 170/938] [D loss: 0.611237] [G loss: 0.588749]\n",
            "[Epoch 0/1] [Batch 171/938] [D loss: 0.598628] [G loss: 0.773604]\n",
            "[Epoch 0/1] [Batch 172/938] [D loss: 0.568124] [G loss: 0.632647]\n",
            "[Epoch 0/1] [Batch 173/938] [D loss: 0.574978] [G loss: 0.790781]\n",
            "[Epoch 0/1] [Batch 174/938] [D loss: 0.553421] [G loss: 0.745441]\n",
            "[Epoch 0/1] [Batch 175/938] [D loss: 0.554647] [G loss: 0.688973]\n",
            "[Epoch 0/1] [Batch 176/938] [D loss: 0.568994] [G loss: 0.893566]\n",
            "[Epoch 0/1] [Batch 177/938] [D loss: 0.582799] [G loss: 0.579197]\n",
            "[Epoch 0/1] [Batch 178/938] [D loss: 0.601387] [G loss: 0.914598]\n",
            "[Epoch 0/1] [Batch 179/938] [D loss: 0.600656] [G loss: 0.550367]\n",
            "[Epoch 0/1] [Batch 180/938] [D loss: 0.563368] [G loss: 0.819908]\n",
            "[Epoch 0/1] [Batch 181/938] [D loss: 0.551038] [G loss: 0.714419]\n",
            "[Epoch 0/1] [Batch 182/938] [D loss: 0.531676] [G loss: 0.813712]\n",
            "[Epoch 0/1] [Batch 183/938] [D loss: 0.560266] [G loss: 0.693394]\n",
            "[Epoch 0/1] [Batch 184/938] [D loss: 0.586271] [G loss: 0.799966]\n",
            "[Epoch 0/1] [Batch 185/938] [D loss: 0.625583] [G loss: 0.513913]\n",
            "[Epoch 0/1] [Batch 186/938] [D loss: 0.679277] [G loss: 0.993202]\n",
            "[Epoch 0/1] [Batch 187/938] [D loss: 0.799862] [G loss: 0.290342]\n",
            "[Epoch 0/1] [Batch 188/938] [D loss: 0.767320] [G loss: 1.221681]\n",
            "[Epoch 0/1] [Batch 189/938] [D loss: 0.799528] [G loss: 0.295983]\n",
            "[Epoch 0/1] [Batch 190/938] [D loss: 0.626169] [G loss: 0.804884]\n",
            "[Epoch 0/1] [Batch 191/938] [D loss: 0.612053] [G loss: 0.730445]\n",
            "[Epoch 0/1] [Batch 192/938] [D loss: 0.620563] [G loss: 0.603536]\n",
            "[Epoch 0/1] [Batch 193/938] [D loss: 0.574551] [G loss: 0.818872]\n",
            "[Epoch 0/1] [Batch 194/938] [D loss: 0.579367] [G loss: 0.695875]\n",
            "[Epoch 0/1] [Batch 195/938] [D loss: 0.573542] [G loss: 0.757706]\n",
            "[Epoch 0/1] [Batch 196/938] [D loss: 0.604928] [G loss: 0.754018]\n",
            "[Epoch 0/1] [Batch 197/938] [D loss: 0.594821] [G loss: 0.594131]\n",
            "[Epoch 0/1] [Batch 198/938] [D loss: 0.597548] [G loss: 0.942242]\n",
            "[Epoch 0/1] [Batch 199/938] [D loss: 0.643938] [G loss: 0.511957]\n",
            "[Epoch 0/1] [Batch 200/938] [D loss: 0.599962] [G loss: 0.867562]\n",
            "[Epoch 0/1] [Batch 201/938] [D loss: 0.588279] [G loss: 0.625693]\n",
            "[Epoch 0/1] [Batch 202/938] [D loss: 0.537530] [G loss: 0.865891]\n",
            "[Epoch 0/1] [Batch 203/938] [D loss: 0.523413] [G loss: 0.761803]\n",
            "[Epoch 0/1] [Batch 204/938] [D loss: 0.506198] [G loss: 0.894270]\n",
            "[Epoch 0/1] [Batch 205/938] [D loss: 0.539644] [G loss: 0.800958]\n",
            "[Epoch 0/1] [Batch 206/938] [D loss: 0.544093] [G loss: 0.786826]\n",
            "[Epoch 0/1] [Batch 207/938] [D loss: 0.557499] [G loss: 0.706253]\n",
            "[Epoch 0/1] [Batch 208/938] [D loss: 0.540955] [G loss: 0.899241]\n",
            "[Epoch 0/1] [Batch 209/938] [D loss: 0.573376] [G loss: 0.630393]\n",
            "[Epoch 0/1] [Batch 210/938] [D loss: 0.547927] [G loss: 0.914803]\n",
            "[Epoch 0/1] [Batch 211/938] [D loss: 0.625466] [G loss: 0.578283]\n",
            "[Epoch 0/1] [Batch 212/938] [D loss: 0.592417] [G loss: 0.941179]\n",
            "[Epoch 0/1] [Batch 213/938] [D loss: 0.696422] [G loss: 0.397976]\n",
            "[Epoch 0/1] [Batch 214/938] [D loss: 0.767886] [G loss: 1.371001]\n",
            "[Epoch 0/1] [Batch 215/938] [D loss: 0.892580] [G loss: 0.222526]\n",
            "[Epoch 0/1] [Batch 216/938] [D loss: 0.556162] [G loss: 0.920947]\n",
            "[Epoch 0/1] [Batch 217/938] [D loss: 0.538546] [G loss: 0.981961]\n",
            "[Epoch 0/1] [Batch 218/938] [D loss: 0.550123] [G loss: 0.548451]\n",
            "[Epoch 0/1] [Batch 219/938] [D loss: 0.489507] [G loss: 1.016804]\n",
            "[Epoch 0/1] [Batch 220/938] [D loss: 0.486293] [G loss: 0.797128]\n",
            "[Epoch 0/1] [Batch 221/938] [D loss: 0.511041] [G loss: 0.801296]\n",
            "[Epoch 0/1] [Batch 222/938] [D loss: 0.531271] [G loss: 0.778070]\n",
            "[Epoch 0/1] [Batch 223/938] [D loss: 0.552060] [G loss: 0.775996]\n",
            "[Epoch 0/1] [Batch 224/938] [D loss: 0.563056] [G loss: 0.675129]\n",
            "[Epoch 0/1] [Batch 225/938] [D loss: 0.555658] [G loss: 0.773793]\n",
            "[Epoch 0/1] [Batch 226/938] [D loss: 0.557946] [G loss: 0.731583]\n",
            "[Epoch 0/1] [Batch 227/938] [D loss: 0.543341] [G loss: 0.693874]\n",
            "[Epoch 0/1] [Batch 228/938] [D loss: 0.513384] [G loss: 0.861973]\n",
            "[Epoch 0/1] [Batch 229/938] [D loss: 0.509766] [G loss: 0.733700]\n",
            "[Epoch 0/1] [Batch 230/938] [D loss: 0.512705] [G loss: 0.861598]\n",
            "[Epoch 0/1] [Batch 231/938] [D loss: 0.499677] [G loss: 0.695579]\n",
            "[Epoch 0/1] [Batch 232/938] [D loss: 0.534261] [G loss: 1.082874]\n",
            "[Epoch 0/1] [Batch 233/938] [D loss: 0.571099] [G loss: 0.476636]\n",
            "[Epoch 0/1] [Batch 234/938] [D loss: 0.467605] [G loss: 1.266755]\n",
            "[Epoch 0/1] [Batch 235/938] [D loss: 0.460921] [G loss: 0.743556]\n",
            "[Epoch 0/1] [Batch 236/938] [D loss: 0.463375] [G loss: 0.750395]\n",
            "[Epoch 0/1] [Batch 237/938] [D loss: 0.480526] [G loss: 1.071512]\n",
            "[Epoch 0/1] [Batch 238/938] [D loss: 0.514218] [G loss: 0.622900]\n",
            "[Epoch 0/1] [Batch 239/938] [D loss: 0.488680] [G loss: 0.870167]\n",
            "[Epoch 0/1] [Batch 240/938] [D loss: 0.477639] [G loss: 0.815203]\n",
            "[Epoch 0/1] [Batch 241/938] [D loss: 0.476730] [G loss: 0.846180]\n",
            "[Epoch 0/1] [Batch 242/938] [D loss: 0.468657] [G loss: 0.851975]\n",
            "[Epoch 0/1] [Batch 243/938] [D loss: 0.480552] [G loss: 0.844102]\n",
            "[Epoch 0/1] [Batch 244/938] [D loss: 0.513781] [G loss: 0.871256]\n",
            "[Epoch 0/1] [Batch 245/938] [D loss: 0.514554] [G loss: 0.647002]\n",
            "[Epoch 0/1] [Batch 246/938] [D loss: 0.636845] [G loss: 1.200122]\n",
            "[Epoch 0/1] [Batch 247/938] [D loss: 0.754943] [G loss: 0.297487]\n",
            "[Epoch 0/1] [Batch 248/938] [D loss: 0.498505] [G loss: 1.219983]\n",
            "[Epoch 0/1] [Batch 249/938] [D loss: 0.445122] [G loss: 0.972028]\n",
            "[Epoch 0/1] [Batch 250/938] [D loss: 0.484886] [G loss: 0.691752]\n",
            "[Epoch 0/1] [Batch 251/938] [D loss: 0.452561] [G loss: 1.159451]\n",
            "[Epoch 0/1] [Batch 252/938] [D loss: 0.428097] [G loss: 0.797901]\n",
            "[Epoch 0/1] [Batch 253/938] [D loss: 0.414927] [G loss: 1.102419]\n",
            "[Epoch 0/1] [Batch 254/938] [D loss: 0.414203] [G loss: 0.909527]\n",
            "[Epoch 0/1] [Batch 255/938] [D loss: 0.427457] [G loss: 0.975391]\n",
            "[Epoch 0/1] [Batch 256/938] [D loss: 0.435266] [G loss: 0.909697]\n",
            "[Epoch 0/1] [Batch 257/938] [D loss: 0.446665] [G loss: 0.891428]\n",
            "[Epoch 0/1] [Batch 258/938] [D loss: 0.473424] [G loss: 0.851538]\n",
            "[Epoch 0/1] [Batch 259/938] [D loss: 0.469560] [G loss: 0.931935]\n",
            "[Epoch 0/1] [Batch 260/938] [D loss: 0.471752] [G loss: 0.807732]\n",
            "[Epoch 0/1] [Batch 261/938] [D loss: 0.490248] [G loss: 1.087566]\n",
            "[Epoch 0/1] [Batch 262/938] [D loss: 0.563329] [G loss: 0.527079]\n",
            "[Epoch 0/1] [Batch 263/938] [D loss: 0.540097] [G loss: 1.383423]\n",
            "[Epoch 0/1] [Batch 264/938] [D loss: 0.606610] [G loss: 0.439240]\n",
            "[Epoch 0/1] [Batch 265/938] [D loss: 0.542917] [G loss: 1.192268]\n",
            "[Epoch 0/1] [Batch 266/938] [D loss: 0.560752] [G loss: 0.563331]\n",
            "[Epoch 0/1] [Batch 267/938] [D loss: 0.502629] [G loss: 0.936864]\n",
            "[Epoch 0/1] [Batch 268/938] [D loss: 0.501971] [G loss: 0.823905]\n",
            "[Epoch 0/1] [Batch 269/938] [D loss: 0.524950] [G loss: 0.772302]\n",
            "[Epoch 0/1] [Batch 270/938] [D loss: 0.538715] [G loss: 0.773526]\n",
            "[Epoch 0/1] [Batch 271/938] [D loss: 0.551957] [G loss: 0.725635]\n",
            "[Epoch 0/1] [Batch 272/938] [D loss: 0.528724] [G loss: 0.833255]\n",
            "[Epoch 0/1] [Batch 273/938] [D loss: 0.508354] [G loss: 0.830551]\n",
            "[Epoch 0/1] [Batch 274/938] [D loss: 0.513733] [G loss: 0.891377]\n",
            "[Epoch 0/1] [Batch 275/938] [D loss: 0.491476] [G loss: 0.750365]\n",
            "[Epoch 0/1] [Batch 276/938] [D loss: 0.473939] [G loss: 1.143733]\n",
            "[Epoch 0/1] [Batch 277/938] [D loss: 0.489600] [G loss: 0.661998]\n",
            "[Epoch 0/1] [Batch 278/938] [D loss: 0.511998] [G loss: 1.360444]\n",
            "[Epoch 0/1] [Batch 279/938] [D loss: 0.575796] [G loss: 0.536608]\n",
            "[Epoch 0/1] [Batch 280/938] [D loss: 0.509857] [G loss: 1.272706]\n",
            "[Epoch 0/1] [Batch 281/938] [D loss: 0.534396] [G loss: 0.624878]\n",
            "[Epoch 0/1] [Batch 282/938] [D loss: 0.482525] [G loss: 1.095972]\n",
            "[Epoch 0/1] [Batch 283/938] [D loss: 0.528533] [G loss: 0.749186]\n",
            "[Epoch 0/1] [Batch 284/938] [D loss: 0.531572] [G loss: 0.842798]\n",
            "[Epoch 0/1] [Batch 285/938] [D loss: 0.546698] [G loss: 0.811153]\n",
            "[Epoch 0/1] [Batch 286/938] [D loss: 0.575576] [G loss: 0.717205]\n",
            "[Epoch 0/1] [Batch 287/938] [D loss: 0.578604] [G loss: 0.799878]\n",
            "[Epoch 0/1] [Batch 288/938] [D loss: 0.580091] [G loss: 0.681397]\n",
            "[Epoch 0/1] [Batch 289/938] [D loss: 0.535372] [G loss: 0.917492]\n",
            "[Epoch 0/1] [Batch 290/938] [D loss: 0.539075] [G loss: 0.688612]\n",
            "[Epoch 0/1] [Batch 291/938] [D loss: 0.530144] [G loss: 1.109058]\n",
            "[Epoch 0/1] [Batch 292/938] [D loss: 0.564949] [G loss: 0.525635]\n",
            "[Epoch 0/1] [Batch 293/938] [D loss: 0.554780] [G loss: 1.559145]\n",
            "[Epoch 0/1] [Batch 294/938] [D loss: 0.630265] [G loss: 0.413304]\n",
            "[Epoch 0/1] [Batch 295/938] [D loss: 0.525845] [G loss: 1.322359]\n",
            "[Epoch 0/1] [Batch 296/938] [D loss: 0.517746] [G loss: 0.610121]\n",
            "[Epoch 0/1] [Batch 297/938] [D loss: 0.485060] [G loss: 0.937881]\n",
            "[Epoch 0/1] [Batch 298/938] [D loss: 0.466148] [G loss: 0.884044]\n",
            "[Epoch 0/1] [Batch 299/938] [D loss: 0.479451] [G loss: 0.842019]\n",
            "[Epoch 0/1] [Batch 300/938] [D loss: 0.503197] [G loss: 0.893367]\n",
            "[Epoch 0/1] [Batch 301/938] [D loss: 0.522562] [G loss: 0.636192]\n",
            "[Epoch 0/1] [Batch 302/938] [D loss: 0.551653] [G loss: 1.256761]\n",
            "[Epoch 0/1] [Batch 303/938] [D loss: 0.669185] [G loss: 0.434075]\n",
            "[Epoch 0/1] [Batch 304/938] [D loss: 0.548680] [G loss: 1.087746]\n",
            "[Epoch 0/1] [Batch 305/938] [D loss: 0.528501] [G loss: 0.663433]\n",
            "[Epoch 0/1] [Batch 306/938] [D loss: 0.523709] [G loss: 0.925578]\n",
            "[Epoch 0/1] [Batch 307/938] [D loss: 0.507275] [G loss: 0.730498]\n",
            "[Epoch 0/1] [Batch 308/938] [D loss: 0.475818] [G loss: 0.966448]\n",
            "[Epoch 0/1] [Batch 309/938] [D loss: 0.516525] [G loss: 0.792798]\n",
            "[Epoch 0/1] [Batch 310/938] [D loss: 0.506332] [G loss: 0.721264]\n",
            "[Epoch 0/1] [Batch 311/938] [D loss: 0.485230] [G loss: 1.053296]\n",
            "[Epoch 0/1] [Batch 312/938] [D loss: 0.552727] [G loss: 0.554424]\n",
            "[Epoch 0/1] [Batch 313/938] [D loss: 0.573837] [G loss: 1.237961]\n",
            "[Epoch 0/1] [Batch 314/938] [D loss: 0.651467] [G loss: 0.383379]\n",
            "[Epoch 0/1] [Batch 315/938] [D loss: 0.545520] [G loss: 1.300424]\n",
            "[Epoch 0/1] [Batch 316/938] [D loss: 0.529138] [G loss: 0.551934]\n",
            "[Epoch 0/1] [Batch 317/938] [D loss: 0.463084] [G loss: 1.117349]\n",
            "[Epoch 0/1] [Batch 318/938] [D loss: 0.437446] [G loss: 0.814555]\n",
            "[Epoch 0/1] [Batch 319/938] [D loss: 0.436117] [G loss: 0.891459]\n",
            "[Epoch 0/1] [Batch 320/938] [D loss: 0.458855] [G loss: 1.012417]\n",
            "[Epoch 0/1] [Batch 321/938] [D loss: 0.486708] [G loss: 0.702832]\n",
            "[Epoch 0/1] [Batch 322/938] [D loss: 0.473911] [G loss: 1.100107]\n",
            "[Epoch 0/1] [Batch 323/938] [D loss: 0.497792] [G loss: 0.686597]\n",
            "[Epoch 0/1] [Batch 324/938] [D loss: 0.499844] [G loss: 1.009267]\n",
            "[Epoch 0/1] [Batch 325/938] [D loss: 0.519387] [G loss: 0.650292]\n",
            "[Epoch 0/1] [Batch 326/938] [D loss: 0.490524] [G loss: 1.100171]\n",
            "[Epoch 0/1] [Batch 327/938] [D loss: 0.521894] [G loss: 0.668964]\n",
            "[Epoch 0/1] [Batch 328/938] [D loss: 0.534856] [G loss: 0.977377]\n",
            "[Epoch 0/1] [Batch 329/938] [D loss: 0.550188] [G loss: 0.611972]\n",
            "[Epoch 0/1] [Batch 330/938] [D loss: 0.536042] [G loss: 0.999735]\n",
            "[Epoch 0/1] [Batch 331/938] [D loss: 0.536023] [G loss: 0.671898]\n",
            "[Epoch 0/1] [Batch 332/938] [D loss: 0.549221] [G loss: 0.901726]\n",
            "[Epoch 0/1] [Batch 333/938] [D loss: 0.530050] [G loss: 0.676790]\n",
            "[Epoch 0/1] [Batch 334/938] [D loss: 0.508984] [G loss: 1.051529]\n",
            "[Epoch 0/1] [Batch 335/938] [D loss: 0.531045] [G loss: 0.663183]\n",
            "[Epoch 0/1] [Batch 336/938] [D loss: 0.565924] [G loss: 1.076319]\n",
            "[Epoch 0/1] [Batch 337/938] [D loss: 0.606841] [G loss: 0.465847]\n",
            "[Epoch 0/1] [Batch 338/938] [D loss: 0.624746] [G loss: 1.453570]\n",
            "[Epoch 0/1] [Batch 339/938] [D loss: 0.670810] [G loss: 0.379196]\n",
            "[Epoch 0/1] [Batch 340/938] [D loss: 0.574724] [G loss: 1.281142]\n",
            "[Epoch 0/1] [Batch 341/938] [D loss: 0.528302] [G loss: 0.653514]\n",
            "[Epoch 0/1] [Batch 342/938] [D loss: 0.518787] [G loss: 0.952558]\n",
            "[Epoch 0/1] [Batch 343/938] [D loss: 0.508881] [G loss: 0.806407]\n",
            "[Epoch 0/1] [Batch 344/938] [D loss: 0.492376] [G loss: 0.954949]\n",
            "[Epoch 0/1] [Batch 345/938] [D loss: 0.495848] [G loss: 0.890912]\n",
            "[Epoch 0/1] [Batch 346/938] [D loss: 0.448795] [G loss: 0.861949]\n",
            "[Epoch 0/1] [Batch 347/938] [D loss: 0.540980] [G loss: 1.139512]\n",
            "[Epoch 0/1] [Batch 348/938] [D loss: 0.603383] [G loss: 0.450114]\n",
            "[Epoch 0/1] [Batch 349/938] [D loss: 0.617774] [G loss: 1.735098]\n",
            "[Epoch 0/1] [Batch 350/938] [D loss: 0.680836] [G loss: 0.349044]\n",
            "[Epoch 0/1] [Batch 351/938] [D loss: 0.451662] [G loss: 1.265983]\n",
            "[Epoch 0/1] [Batch 352/938] [D loss: 0.486750] [G loss: 0.990021]\n",
            "[Epoch 0/1] [Batch 353/938] [D loss: 0.553731] [G loss: 0.582780]\n",
            "[Epoch 0/1] [Batch 354/938] [D loss: 0.552128] [G loss: 1.148860]\n",
            "[Epoch 0/1] [Batch 355/938] [D loss: 0.571679] [G loss: 0.639014]\n",
            "[Epoch 0/1] [Batch 356/938] [D loss: 0.552731] [G loss: 0.897754]\n",
            "[Epoch 0/1] [Batch 357/938] [D loss: 0.576590] [G loss: 0.743910]\n",
            "[Epoch 0/1] [Batch 358/938] [D loss: 0.554471] [G loss: 0.773593]\n",
            "[Epoch 0/1] [Batch 359/938] [D loss: 0.535900] [G loss: 0.959621]\n",
            "[Epoch 0/1] [Batch 360/938] [D loss: 0.530670] [G loss: 0.716930]\n",
            "[Epoch 0/1] [Batch 361/938] [D loss: 0.492327] [G loss: 1.106600]\n",
            "[Epoch 0/1] [Batch 362/938] [D loss: 0.527198] [G loss: 0.640349]\n",
            "[Epoch 0/1] [Batch 363/938] [D loss: 0.506448] [G loss: 1.362705]\n",
            "[Epoch 0/1] [Batch 364/938] [D loss: 0.604555] [G loss: 0.444746]\n",
            "[Epoch 0/1] [Batch 365/938] [D loss: 0.545331] [G loss: 1.607399]\n",
            "[Epoch 0/1] [Batch 366/938] [D loss: 0.593609] [G loss: 0.477923]\n",
            "[Epoch 0/1] [Batch 367/938] [D loss: 0.505720] [G loss: 1.300939]\n",
            "[Epoch 0/1] [Batch 368/938] [D loss: 0.536827] [G loss: 0.636287]\n",
            "[Epoch 0/1] [Batch 369/938] [D loss: 0.514521] [G loss: 1.131313]\n",
            "[Epoch 0/1] [Batch 370/938] [D loss: 0.565805] [G loss: 0.596125]\n",
            "[Epoch 0/1] [Batch 371/938] [D loss: 0.552611] [G loss: 1.240618]\n",
            "[Epoch 0/1] [Batch 372/938] [D loss: 0.611447] [G loss: 0.483112]\n",
            "[Epoch 0/1] [Batch 373/938] [D loss: 0.640804] [G loss: 1.303121]\n",
            "[Epoch 0/1] [Batch 374/938] [D loss: 0.707851] [G loss: 0.342071]\n",
            "[Epoch 0/1] [Batch 375/938] [D loss: 0.596010] [G loss: 1.280150]\n",
            "[Epoch 0/1] [Batch 376/938] [D loss: 0.554878] [G loss: 0.535796]\n",
            "[Epoch 0/1] [Batch 377/938] [D loss: 0.507769] [G loss: 1.025498]\n",
            "[Epoch 0/1] [Batch 378/938] [D loss: 0.563015] [G loss: 0.762247]\n",
            "[Epoch 0/1] [Batch 379/938] [D loss: 0.557074] [G loss: 0.646718]\n",
            "[Epoch 0/1] [Batch 380/938] [D loss: 0.519761] [G loss: 1.030435]\n",
            "[Epoch 0/1] [Batch 381/938] [D loss: 0.533760] [G loss: 0.637816]\n",
            "[Epoch 0/1] [Batch 382/938] [D loss: 0.484377] [G loss: 1.010006]\n",
            "[Epoch 0/1] [Batch 383/938] [D loss: 0.493905] [G loss: 0.794666]\n",
            "[Epoch 0/1] [Batch 384/938] [D loss: 0.509704] [G loss: 0.841449]\n",
            "[Epoch 0/1] [Batch 385/938] [D loss: 0.524625] [G loss: 0.829491]\n",
            "[Epoch 0/1] [Batch 386/938] [D loss: 0.531025] [G loss: 0.707771]\n",
            "[Epoch 0/1] [Batch 387/938] [D loss: 0.568221] [G loss: 1.005793]\n",
            "[Epoch 0/1] [Batch 388/938] [D loss: 0.578676] [G loss: 0.503612]\n",
            "[Epoch 0/1] [Batch 389/938] [D loss: 0.523540] [G loss: 1.213585]\n",
            "[Epoch 0/1] [Batch 390/938] [D loss: 0.562357] [G loss: 0.565192]\n",
            "[Epoch 0/1] [Batch 391/938] [D loss: 0.504924] [G loss: 0.946122]\n",
            "[Epoch 0/1] [Batch 392/938] [D loss: 0.515821] [G loss: 0.787343]\n",
            "[Epoch 0/1] [Batch 393/938] [D loss: 0.529085] [G loss: 0.696275]\n",
            "[Epoch 0/1] [Batch 394/938] [D loss: 0.517400] [G loss: 0.988826]\n",
            "[Epoch 0/1] [Batch 395/938] [D loss: 0.566886] [G loss: 0.592415]\n",
            "[Epoch 0/1] [Batch 396/938] [D loss: 0.540447] [G loss: 0.939314]\n",
            "[Epoch 0/1] [Batch 397/938] [D loss: 0.538703] [G loss: 0.624194]\n",
            "[Epoch 0/1] [Batch 398/938] [D loss: 0.527858] [G loss: 1.063656]\n",
            "[Epoch 0/1] [Batch 399/938] [D loss: 0.562978] [G loss: 0.559805]\n",
            "[Epoch 0/1] [Batch 400/938] [D loss: 0.503278] [G loss: 1.115764]\n",
            "[Epoch 0/1] [Batch 401/938] [D loss: 0.463989] [G loss: 0.701418]\n",
            "[Epoch 0/1] [Batch 402/938] [D loss: 0.480186] [G loss: 1.109046]\n",
            "[Epoch 0/1] [Batch 403/938] [D loss: 0.445844] [G loss: 0.737217]\n",
            "[Epoch 0/1] [Batch 404/938] [D loss: 0.439991] [G loss: 1.200625]\n",
            "[Epoch 0/1] [Batch 405/938] [D loss: 0.466005] [G loss: 0.767813]\n",
            "[Epoch 0/1] [Batch 406/938] [D loss: 0.415951] [G loss: 1.067595]\n",
            "[Epoch 0/1] [Batch 407/938] [D loss: 0.432263] [G loss: 0.896031]\n",
            "[Epoch 0/1] [Batch 408/938] [D loss: 0.423400] [G loss: 0.959005]\n",
            "[Epoch 0/1] [Batch 409/938] [D loss: 0.421789] [G loss: 0.948448]\n",
            "[Epoch 0/1] [Batch 410/938] [D loss: 0.438444] [G loss: 0.938914]\n",
            "[Epoch 0/1] [Batch 411/938] [D loss: 0.454985] [G loss: 0.896991]\n",
            "[Epoch 0/1] [Batch 412/938] [D loss: 0.465255] [G loss: 0.904074]\n",
            "[Epoch 0/1] [Batch 413/938] [D loss: 0.451498] [G loss: 0.874870]\n",
            "[Epoch 0/1] [Batch 414/938] [D loss: 0.459201] [G loss: 1.090772]\n",
            "[Epoch 0/1] [Batch 415/938] [D loss: 0.535668] [G loss: 0.573172]\n",
            "[Epoch 0/1] [Batch 416/938] [D loss: 0.642462] [G loss: 1.860880]\n",
            "[Epoch 0/1] [Batch 417/938] [D loss: 0.962659] [G loss: 0.173715]\n",
            "[Epoch 0/1] [Batch 418/938] [D loss: 0.414261] [G loss: 1.519985]\n",
            "[Epoch 0/1] [Batch 419/938] [D loss: 0.356119] [G loss: 1.283913]\n",
            "[Epoch 0/1] [Batch 420/938] [D loss: 0.418067] [G loss: 0.757153]\n",
            "[Epoch 0/1] [Batch 421/938] [D loss: 0.404485] [G loss: 1.391369]\n",
            "[Epoch 0/1] [Batch 422/938] [D loss: 0.443715] [G loss: 0.797222]\n",
            "[Epoch 0/1] [Batch 423/938] [D loss: 0.441462] [G loss: 1.089623]\n",
            "[Epoch 0/1] [Batch 424/938] [D loss: 0.454228] [G loss: 0.856567]\n",
            "[Epoch 0/1] [Batch 425/938] [D loss: 0.474977] [G loss: 0.935798]\n",
            "[Epoch 0/1] [Batch 426/938] [D loss: 0.500185] [G loss: 0.846331]\n",
            "[Epoch 0/1] [Batch 427/938] [D loss: 0.479906] [G loss: 1.024129]\n",
            "[Epoch 0/1] [Batch 428/938] [D loss: 0.512968] [G loss: 0.719574]\n",
            "[Epoch 0/1] [Batch 429/938] [D loss: 0.562045] [G loss: 1.111245]\n",
            "[Epoch 0/1] [Batch 430/938] [D loss: 0.653244] [G loss: 0.390001]\n",
            "[Epoch 0/1] [Batch 431/938] [D loss: 0.634944] [G loss: 1.864179]\n",
            "[Epoch 0/1] [Batch 432/938] [D loss: 0.725535] [G loss: 0.319138]\n",
            "[Epoch 0/1] [Batch 433/938] [D loss: 0.386585] [G loss: 1.303928]\n",
            "[Epoch 0/1] [Batch 434/938] [D loss: 0.398714] [G loss: 1.462961]\n",
            "[Epoch 0/1] [Batch 435/938] [D loss: 0.469742] [G loss: 0.661771]\n",
            "[Epoch 0/1] [Batch 436/938] [D loss: 0.391252] [G loss: 1.411453]\n",
            "[Epoch 0/1] [Batch 437/938] [D loss: 0.414416] [G loss: 0.965912]\n",
            "[Epoch 0/1] [Batch 438/938] [D loss: 0.403336] [G loss: 1.146912]\n",
            "[Epoch 0/1] [Batch 439/938] [D loss: 0.419591] [G loss: 1.056193]\n",
            "[Epoch 0/1] [Batch 440/938] [D loss: 0.441951] [G loss: 0.963800]\n",
            "[Epoch 0/1] [Batch 441/938] [D loss: 0.457940] [G loss: 1.046358]\n",
            "[Epoch 0/1] [Batch 442/938] [D loss: 0.475486] [G loss: 0.814308]\n",
            "[Epoch 0/1] [Batch 443/938] [D loss: 0.472240] [G loss: 1.184417]\n",
            "[Epoch 0/1] [Batch 444/938] [D loss: 0.541156] [G loss: 0.577349]\n",
            "[Epoch 0/1] [Batch 445/938] [D loss: 0.569221] [G loss: 1.677998]\n",
            "[Epoch 0/1] [Batch 446/938] [D loss: 0.755763] [G loss: 0.291879]\n",
            "[Epoch 0/1] [Batch 447/938] [D loss: 0.598521] [G loss: 1.795742]\n",
            "[Epoch 0/1] [Batch 448/938] [D loss: 0.567021] [G loss: 0.525640]\n",
            "[Epoch 0/1] [Batch 449/938] [D loss: 0.430602] [G loss: 1.126225]\n",
            "[Epoch 0/1] [Batch 450/938] [D loss: 0.463275] [G loss: 1.080650]\n",
            "[Epoch 0/1] [Batch 451/938] [D loss: 0.534887] [G loss: 0.701847]\n",
            "[Epoch 0/1] [Batch 452/938] [D loss: 0.473893] [G loss: 1.112367]\n",
            "[Epoch 0/1] [Batch 453/938] [D loss: 0.486211] [G loss: 0.871884]\n",
            "[Epoch 0/1] [Batch 454/938] [D loss: 0.477866] [G loss: 0.931077]\n",
            "[Epoch 0/1] [Batch 455/938] [D loss: 0.477943] [G loss: 1.021298]\n",
            "[Epoch 0/1] [Batch 456/938] [D loss: 0.492317] [G loss: 0.871885]\n",
            "[Epoch 0/1] [Batch 457/938] [D loss: 0.480558] [G loss: 0.966509]\n",
            "[Epoch 0/1] [Batch 458/938] [D loss: 0.486073] [G loss: 0.916179]\n",
            "[Epoch 0/1] [Batch 459/938] [D loss: 0.472657] [G loss: 0.971302]\n",
            "[Epoch 0/1] [Batch 460/938] [D loss: 0.443715] [G loss: 0.937714]\n",
            "[Epoch 0/1] [Batch 461/938] [D loss: 0.402195] [G loss: 1.290523]\n",
            "[Epoch 0/1] [Batch 462/938] [D loss: 0.449875] [G loss: 0.845985]\n",
            "[Epoch 0/1] [Batch 463/938] [D loss: 0.406522] [G loss: 1.240688]\n",
            "[Epoch 0/1] [Batch 464/938] [D loss: 0.424339] [G loss: 0.889636]\n",
            "[Epoch 0/1] [Batch 465/938] [D loss: 0.461067] [G loss: 1.316326]\n",
            "[Epoch 0/1] [Batch 466/938] [D loss: 0.570194] [G loss: 0.505501]\n",
            "[Epoch 0/1] [Batch 467/938] [D loss: 0.685201] [G loss: 2.139780]\n",
            "[Epoch 0/1] [Batch 468/938] [D loss: 0.905909] [G loss: 0.215405]\n",
            "[Epoch 0/1] [Batch 469/938] [D loss: 0.484445] [G loss: 1.249242]\n",
            "[Epoch 0/1] [Batch 470/938] [D loss: 0.490177] [G loss: 1.133928]\n",
            "[Epoch 0/1] [Batch 471/938] [D loss: 0.527535] [G loss: 0.573935]\n",
            "[Epoch 0/1] [Batch 472/938] [D loss: 0.392664] [G loss: 1.369105]\n",
            "[Epoch 0/1] [Batch 473/938] [D loss: 0.436945] [G loss: 1.076321]\n",
            "[Epoch 0/1] [Batch 474/938] [D loss: 0.502148] [G loss: 0.671449]\n",
            "[Epoch 0/1] [Batch 475/938] [D loss: 0.535692] [G loss: 1.366783]\n",
            "[Epoch 0/1] [Batch 476/938] [D loss: 0.638169] [G loss: 0.441319]\n",
            "[Epoch 0/1] [Batch 477/938] [D loss: 0.529906] [G loss: 1.548714]\n",
            "[Epoch 0/1] [Batch 478/938] [D loss: 0.534533] [G loss: 0.646592]\n",
            "[Epoch 0/1] [Batch 479/938] [D loss: 0.499124] [G loss: 1.043325]\n",
            "[Epoch 0/1] [Batch 480/938] [D loss: 0.509534] [G loss: 0.800970]\n",
            "[Epoch 0/1] [Batch 481/938] [D loss: 0.448922] [G loss: 1.028560]\n",
            "[Epoch 0/1] [Batch 482/938] [D loss: 0.471197] [G loss: 1.002729]\n",
            "[Epoch 0/1] [Batch 483/938] [D loss: 0.448270] [G loss: 0.784876]\n",
            "[Epoch 0/1] [Batch 484/938] [D loss: 0.398981] [G loss: 1.543661]\n",
            "[Epoch 0/1] [Batch 485/938] [D loss: 0.470591] [G loss: 0.694191]\n",
            "[Epoch 0/1] [Batch 486/938] [D loss: 0.404251] [G loss: 1.368240]\n",
            "[Epoch 0/1] [Batch 487/938] [D loss: 0.436750] [G loss: 0.800590]\n",
            "[Epoch 0/1] [Batch 488/938] [D loss: 0.425104] [G loss: 1.213965]\n",
            "[Epoch 0/1] [Batch 489/938] [D loss: 0.477296] [G loss: 0.806222]\n",
            "[Epoch 0/1] [Batch 490/938] [D loss: 0.465399] [G loss: 0.972119]\n",
            "[Epoch 0/1] [Batch 491/938] [D loss: 0.484865] [G loss: 0.904052]\n",
            "[Epoch 0/1] [Batch 492/938] [D loss: 0.497900] [G loss: 0.893121]\n",
            "[Epoch 0/1] [Batch 493/938] [D loss: 0.498395] [G loss: 0.840860]\n",
            "[Epoch 0/1] [Batch 494/938] [D loss: 0.476982] [G loss: 1.023770]\n",
            "[Epoch 0/1] [Batch 495/938] [D loss: 0.478551] [G loss: 0.758513]\n",
            "[Epoch 0/1] [Batch 496/938] [D loss: 0.558847] [G loss: 1.262109]\n",
            "[Epoch 0/1] [Batch 497/938] [D loss: 0.690070] [G loss: 0.379913]\n",
            "[Epoch 0/1] [Batch 498/938] [D loss: 0.594997] [G loss: 1.691644]\n",
            "[Epoch 0/1] [Batch 499/938] [D loss: 0.558281] [G loss: 0.488520]\n",
            "[Epoch 0/1] [Batch 500/938] [D loss: 0.477217] [G loss: 1.026079]\n",
            "[Epoch 0/1] [Batch 501/938] [D loss: 0.457419] [G loss: 0.936891]\n",
            "[Epoch 0/1] [Batch 502/938] [D loss: 0.463961] [G loss: 0.783952]\n",
            "[Epoch 0/1] [Batch 503/938] [D loss: 0.449589] [G loss: 1.162060]\n",
            "[Epoch 0/1] [Batch 504/938] [D loss: 0.498014] [G loss: 0.703602]\n",
            "[Epoch 0/1] [Batch 505/938] [D loss: 0.452840] [G loss: 1.069548]\n",
            "[Epoch 0/1] [Batch 506/938] [D loss: 0.438942] [G loss: 0.756289]\n",
            "[Epoch 0/1] [Batch 507/938] [D loss: 0.472902] [G loss: 1.303763]\n",
            "[Epoch 0/1] [Batch 508/938] [D loss: 0.537314] [G loss: 0.548087]\n",
            "[Epoch 0/1] [Batch 509/938] [D loss: 0.535483] [G loss: 1.709054]\n",
            "[Epoch 0/1] [Batch 510/938] [D loss: 0.587850] [G loss: 0.455626]\n",
            "[Epoch 0/1] [Batch 511/938] [D loss: 0.484749] [G loss: 1.496742]\n",
            "[Epoch 0/1] [Batch 512/938] [D loss: 0.421024] [G loss: 0.809838]\n",
            "[Epoch 0/1] [Batch 513/938] [D loss: 0.447941] [G loss: 1.146817]\n",
            "[Epoch 0/1] [Batch 514/938] [D loss: 0.461855] [G loss: 0.843162]\n",
            "[Epoch 0/1] [Batch 515/938] [D loss: 0.388682] [G loss: 1.079828]\n",
            "[Epoch 0/1] [Batch 516/938] [D loss: 0.431775] [G loss: 1.104599]\n",
            "[Epoch 0/1] [Batch 517/938] [D loss: 0.448129] [G loss: 0.781798]\n",
            "[Epoch 0/1] [Batch 518/938] [D loss: 0.434531] [G loss: 1.415236]\n",
            "[Epoch 0/1] [Batch 519/938] [D loss: 0.475573] [G loss: 0.666202]\n",
            "[Epoch 0/1] [Batch 520/938] [D loss: 0.475793] [G loss: 1.574976]\n",
            "[Epoch 0/1] [Batch 521/938] [D loss: 0.564483] [G loss: 0.554279]\n",
            "[Epoch 0/1] [Batch 522/938] [D loss: 0.440444] [G loss: 1.343491]\n",
            "[Epoch 0/1] [Batch 523/938] [D loss: 0.490226] [G loss: 0.795135]\n",
            "[Epoch 0/1] [Batch 524/938] [D loss: 0.430949] [G loss: 1.039080]\n",
            "[Epoch 0/1] [Batch 525/938] [D loss: 0.451299] [G loss: 1.001732]\n",
            "[Epoch 0/1] [Batch 526/938] [D loss: 0.451339] [G loss: 0.879201]\n",
            "[Epoch 0/1] [Batch 527/938] [D loss: 0.445209] [G loss: 1.186955]\n",
            "[Epoch 0/1] [Batch 528/938] [D loss: 0.453693] [G loss: 0.792252]\n",
            "[Epoch 0/1] [Batch 529/938] [D loss: 0.454138] [G loss: 1.263097]\n",
            "[Epoch 0/1] [Batch 530/938] [D loss: 0.479969] [G loss: 0.648814]\n",
            "[Epoch 0/1] [Batch 531/938] [D loss: 0.482261] [G loss: 1.519564]\n",
            "[Epoch 0/1] [Batch 532/938] [D loss: 0.558611] [G loss: 0.518737]\n",
            "[Epoch 0/1] [Batch 533/938] [D loss: 0.496293] [G loss: 1.547995]\n",
            "[Epoch 0/1] [Batch 534/938] [D loss: 0.538482] [G loss: 0.567214]\n",
            "[Epoch 0/1] [Batch 535/938] [D loss: 0.451738] [G loss: 1.486123]\n",
            "[Epoch 0/1] [Batch 536/938] [D loss: 0.483556] [G loss: 0.753011]\n",
            "[Epoch 0/1] [Batch 537/938] [D loss: 0.477628] [G loss: 1.154998]\n",
            "[Epoch 0/1] [Batch 538/938] [D loss: 0.497811] [G loss: 0.839152]\n",
            "[Epoch 0/1] [Batch 539/938] [D loss: 0.449396] [G loss: 1.039172]\n",
            "[Epoch 0/1] [Batch 540/938] [D loss: 0.447186] [G loss: 0.983765]\n",
            "[Epoch 0/1] [Batch 541/938] [D loss: 0.476186] [G loss: 0.896004]\n",
            "[Epoch 0/1] [Batch 542/938] [D loss: 0.478694] [G loss: 1.167438]\n",
            "[Epoch 0/1] [Batch 543/938] [D loss: 0.585366] [G loss: 0.563344]\n",
            "[Epoch 0/1] [Batch 544/938] [D loss: 0.651963] [G loss: 1.806137]\n",
            "[Epoch 0/1] [Batch 545/938] [D loss: 0.861120] [G loss: 0.250144]\n",
            "[Epoch 0/1] [Batch 546/938] [D loss: 0.536495] [G loss: 1.561654]\n",
            "[Epoch 0/1] [Batch 547/938] [D loss: 0.478476] [G loss: 0.915823]\n",
            "[Epoch 0/1] [Batch 548/938] [D loss: 0.509010] [G loss: 0.770524]\n",
            "[Epoch 0/1] [Batch 549/938] [D loss: 0.519800] [G loss: 1.119840]\n",
            "[Epoch 0/1] [Batch 550/938] [D loss: 0.539538] [G loss: 0.644065]\n",
            "[Epoch 0/1] [Batch 551/938] [D loss: 0.510899] [G loss: 1.324389]\n",
            "[Epoch 0/1] [Batch 552/938] [D loss: 0.592301] [G loss: 0.590715]\n",
            "[Epoch 0/1] [Batch 553/938] [D loss: 0.528775] [G loss: 1.202960]\n",
            "[Epoch 0/1] [Batch 554/938] [D loss: 0.547355] [G loss: 0.693267]\n",
            "[Epoch 0/1] [Batch 555/938] [D loss: 0.480539] [G loss: 1.167055]\n",
            "[Epoch 0/1] [Batch 556/938] [D loss: 0.490491] [G loss: 0.879611]\n",
            "[Epoch 0/1] [Batch 557/938] [D loss: 0.437419] [G loss: 1.010398]\n",
            "[Epoch 0/1] [Batch 558/938] [D loss: 0.482758] [G loss: 1.093387]\n",
            "[Epoch 0/1] [Batch 559/938] [D loss: 0.490995] [G loss: 0.722803]\n",
            "[Epoch 0/1] [Batch 560/938] [D loss: 0.487503] [G loss: 1.470188]\n",
            "[Epoch 0/1] [Batch 561/938] [D loss: 0.553504] [G loss: 0.555987]\n",
            "[Epoch 0/1] [Batch 562/938] [D loss: 0.504321] [G loss: 1.639009]\n",
            "[Epoch 0/1] [Batch 563/938] [D loss: 0.569380] [G loss: 0.518286]\n",
            "[Epoch 0/1] [Batch 564/938] [D loss: 0.478068] [G loss: 1.523170]\n",
            "[Epoch 0/1] [Batch 565/938] [D loss: 0.496856] [G loss: 0.630709]\n",
            "[Epoch 0/1] [Batch 566/938] [D loss: 0.457622] [G loss: 1.418865]\n",
            "[Epoch 0/1] [Batch 567/938] [D loss: 0.487786] [G loss: 0.750061]\n",
            "[Epoch 0/1] [Batch 568/938] [D loss: 0.470769] [G loss: 0.990552]\n",
            "[Epoch 0/1] [Batch 569/938] [D loss: 0.475494] [G loss: 0.849424]\n",
            "[Epoch 0/1] [Batch 570/938] [D loss: 0.502268] [G loss: 1.140862]\n",
            "[Epoch 0/1] [Batch 571/938] [D loss: 0.580287] [G loss: 0.523002]\n",
            "[Epoch 0/1] [Batch 572/938] [D loss: 0.701833] [G loss: 1.632358]\n",
            "[Epoch 0/1] [Batch 573/938] [D loss: 0.825544] [G loss: 0.257739]\n",
            "[Epoch 0/1] [Batch 574/938] [D loss: 0.520166] [G loss: 1.461897]\n",
            "[Epoch 0/1] [Batch 575/938] [D loss: 0.474527] [G loss: 0.908139]\n",
            "[Epoch 0/1] [Batch 576/938] [D loss: 0.486490] [G loss: 0.759387]\n",
            "[Epoch 0/1] [Batch 577/938] [D loss: 0.453665] [G loss: 1.255375]\n",
            "[Epoch 0/1] [Batch 578/938] [D loss: 0.452614] [G loss: 0.720640]\n",
            "[Epoch 0/1] [Batch 579/938] [D loss: 0.476427] [G loss: 1.441551]\n",
            "[Epoch 0/1] [Batch 580/938] [D loss: 0.507457] [G loss: 0.611236]\n",
            "[Epoch 0/1] [Batch 581/938] [D loss: 0.461106] [G loss: 1.431763]\n",
            "[Epoch 0/1] [Batch 582/938] [D loss: 0.440155] [G loss: 0.712943]\n",
            "[Epoch 0/1] [Batch 583/938] [D loss: 0.373928] [G loss: 1.407846]\n",
            "[Epoch 0/1] [Batch 584/938] [D loss: 0.416533] [G loss: 0.979651]\n",
            "[Epoch 0/1] [Batch 585/938] [D loss: 0.431627] [G loss: 0.880015]\n",
            "[Epoch 0/1] [Batch 586/938] [D loss: 0.430175] [G loss: 1.186645]\n",
            "[Epoch 0/1] [Batch 587/938] [D loss: 0.481711] [G loss: 0.707893]\n",
            "[Epoch 0/1] [Batch 588/938] [D loss: 0.559920] [G loss: 1.498504]\n",
            "[Epoch 0/1] [Batch 589/938] [D loss: 0.801711] [G loss: 0.285885]\n",
            "[Epoch 0/1] [Batch 590/938] [D loss: 0.701280] [G loss: 1.898941]\n",
            "[Epoch 0/1] [Batch 591/938] [D loss: 0.715022] [G loss: 0.346021]\n",
            "[Epoch 0/1] [Batch 592/938] [D loss: 0.454049] [G loss: 1.193721]\n",
            "[Epoch 0/1] [Batch 593/938] [D loss: 0.423569] [G loss: 1.098238]\n",
            "[Epoch 0/1] [Batch 594/938] [D loss: 0.453985] [G loss: 0.747339]\n",
            "[Epoch 0/1] [Batch 595/938] [D loss: 0.441692] [G loss: 1.253768]\n",
            "[Epoch 0/1] [Batch 596/938] [D loss: 0.458409] [G loss: 0.771046]\n",
            "[Epoch 0/1] [Batch 597/938] [D loss: 0.438392] [G loss: 1.063437]\n",
            "[Epoch 0/1] [Batch 598/938] [D loss: 0.471537] [G loss: 0.930366]\n",
            "[Epoch 0/1] [Batch 599/938] [D loss: 0.467834] [G loss: 0.913052]\n",
            "[Epoch 0/1] [Batch 600/938] [D loss: 0.441404] [G loss: 0.905010]\n",
            "[Epoch 0/1] [Batch 601/938] [D loss: 0.424545] [G loss: 1.199041]\n",
            "[Epoch 0/1] [Batch 602/938] [D loss: 0.473814] [G loss: 0.767305]\n",
            "[Epoch 0/1] [Batch 603/938] [D loss: 0.454016] [G loss: 1.254061]\n",
            "[Epoch 0/1] [Batch 604/938] [D loss: 0.463423] [G loss: 0.675175]\n",
            "[Epoch 0/1] [Batch 605/938] [D loss: 0.433812] [G loss: 1.680747]\n",
            "[Epoch 0/1] [Batch 606/938] [D loss: 0.491259] [G loss: 0.641524]\n",
            "[Epoch 0/1] [Batch 607/938] [D loss: 0.363661] [G loss: 1.409639]\n",
            "[Epoch 0/1] [Batch 608/938] [D loss: 0.405601] [G loss: 1.075192]\n",
            "[Epoch 0/1] [Batch 609/938] [D loss: 0.427037] [G loss: 0.871968]\n",
            "[Epoch 0/1] [Batch 610/938] [D loss: 0.428235] [G loss: 1.413566]\n",
            "[Epoch 0/1] [Batch 611/938] [D loss: 0.508306] [G loss: 0.649954]\n",
            "[Epoch 0/1] [Batch 612/938] [D loss: 0.468940] [G loss: 1.605121]\n",
            "[Epoch 0/1] [Batch 613/938] [D loss: 0.532836] [G loss: 0.549293]\n",
            "[Epoch 0/1] [Batch 614/938] [D loss: 0.529737] [G loss: 1.776067]\n",
            "[Epoch 0/1] [Batch 615/938] [D loss: 0.627813] [G loss: 0.387652]\n",
            "[Epoch 0/1] [Batch 616/938] [D loss: 0.476313] [G loss: 2.110830]\n",
            "[Epoch 0/1] [Batch 617/938] [D loss: 0.426655] [G loss: 0.686483]\n",
            "[Epoch 0/1] [Batch 618/938] [D loss: 0.377208] [G loss: 1.492824]\n",
            "[Epoch 0/1] [Batch 619/938] [D loss: 0.370247] [G loss: 0.937064]\n",
            "[Epoch 0/1] [Batch 620/938] [D loss: 0.390732] [G loss: 1.336605]\n",
            "[Epoch 0/1] [Batch 621/938] [D loss: 0.443135] [G loss: 0.810353]\n",
            "[Epoch 0/1] [Batch 622/938] [D loss: 0.395782] [G loss: 1.330106]\n",
            "[Epoch 0/1] [Batch 623/938] [D loss: 0.486001] [G loss: 0.813998]\n",
            "[Epoch 0/1] [Batch 624/938] [D loss: 0.439135] [G loss: 1.178602]\n",
            "[Epoch 0/1] [Batch 625/938] [D loss: 0.471599] [G loss: 0.770372]\n",
            "[Epoch 0/1] [Batch 626/938] [D loss: 0.474644] [G loss: 1.436712]\n",
            "[Epoch 0/1] [Batch 627/938] [D loss: 0.550898] [G loss: 0.484985]\n",
            "[Epoch 0/1] [Batch 628/938] [D loss: 0.677479] [G loss: 2.312582]\n",
            "[Epoch 0/1] [Batch 629/938] [D loss: 0.823798] [G loss: 0.228810]\n",
            "[Epoch 0/1] [Batch 630/938] [D loss: 0.390643] [G loss: 1.676437]\n",
            "[Epoch 0/1] [Batch 631/938] [D loss: 0.371853] [G loss: 1.308377]\n",
            "[Epoch 0/1] [Batch 632/938] [D loss: 0.411029] [G loss: 0.719620]\n",
            "[Epoch 0/1] [Batch 633/938] [D loss: 0.432824] [G loss: 1.804626]\n",
            "[Epoch 0/1] [Batch 634/938] [D loss: 0.457785] [G loss: 0.644419]\n",
            "[Epoch 0/1] [Batch 635/938] [D loss: 0.385160] [G loss: 1.694269]\n",
            "[Epoch 0/1] [Batch 636/938] [D loss: 0.395586] [G loss: 0.899858]\n",
            "[Epoch 0/1] [Batch 637/938] [D loss: 0.408174] [G loss: 1.332426]\n",
            "[Epoch 0/1] [Batch 638/938] [D loss: 0.452131] [G loss: 0.803614]\n",
            "[Epoch 0/1] [Batch 639/938] [D loss: 0.455132] [G loss: 1.447687]\n",
            "[Epoch 0/1] [Batch 640/938] [D loss: 0.552280] [G loss: 0.519559]\n",
            "[Epoch 0/1] [Batch 641/938] [D loss: 0.632051] [G loss: 2.243700]\n",
            "[Epoch 0/1] [Batch 642/938] [D loss: 0.756754] [G loss: 0.296999]\n",
            "[Epoch 0/1] [Batch 643/938] [D loss: 0.528754] [G loss: 1.845002]\n",
            "[Epoch 0/1] [Batch 644/938] [D loss: 0.424202] [G loss: 0.779152]\n",
            "[Epoch 0/1] [Batch 645/938] [D loss: 0.373814] [G loss: 1.362395]\n",
            "[Epoch 0/1] [Batch 646/938] [D loss: 0.357360] [G loss: 1.139308]\n",
            "[Epoch 0/1] [Batch 647/938] [D loss: 0.346560] [G loss: 1.124222]\n",
            "[Epoch 0/1] [Batch 648/938] [D loss: 0.319430] [G loss: 1.359089]\n",
            "[Epoch 0/1] [Batch 649/938] [D loss: 0.364337] [G loss: 1.264724]\n",
            "[Epoch 0/1] [Batch 650/938] [D loss: 0.391441] [G loss: 1.015177]\n",
            "[Epoch 0/1] [Batch 651/938] [D loss: 0.357702] [G loss: 1.320078]\n",
            "[Epoch 0/1] [Batch 652/938] [D loss: 0.411270] [G loss: 0.971326]\n",
            "[Epoch 0/1] [Batch 653/938] [D loss: 0.430081] [G loss: 1.242294]\n",
            "[Epoch 0/1] [Batch 654/938] [D loss: 0.481927] [G loss: 0.684895]\n",
            "[Epoch 0/1] [Batch 655/938] [D loss: 0.588933] [G loss: 2.033373]\n",
            "[Epoch 0/1] [Batch 656/938] [D loss: 0.982384] [G loss: 0.178561]\n",
            "[Epoch 0/1] [Batch 657/938] [D loss: 0.562071] [G loss: 2.192729]\n",
            "[Epoch 0/1] [Batch 658/938] [D loss: 0.490507] [G loss: 0.836084]\n",
            "[Epoch 0/1] [Batch 659/938] [D loss: 0.431813] [G loss: 1.099381]\n",
            "[Epoch 0/1] [Batch 660/938] [D loss: 0.399467] [G loss: 1.145162]\n",
            "[Epoch 0/1] [Batch 661/938] [D loss: 0.393295] [G loss: 1.074768]\n",
            "[Epoch 0/1] [Batch 662/938] [D loss: 0.413969] [G loss: 1.121397]\n",
            "[Epoch 0/1] [Batch 663/938] [D loss: 0.458264] [G loss: 1.061277]\n",
            "[Epoch 0/1] [Batch 664/938] [D loss: 0.492844] [G loss: 0.936599]\n",
            "[Epoch 0/1] [Batch 665/938] [D loss: 0.484662] [G loss: 0.987267]\n",
            "[Epoch 0/1] [Batch 666/938] [D loss: 0.489981] [G loss: 0.942559]\n",
            "[Epoch 0/1] [Batch 667/938] [D loss: 0.469228] [G loss: 1.033824]\n",
            "[Epoch 0/1] [Batch 668/938] [D loss: 0.450877] [G loss: 0.928246]\n",
            "[Epoch 0/1] [Batch 669/938] [D loss: 0.423379] [G loss: 1.329256]\n",
            "[Epoch 0/1] [Batch 670/938] [D loss: 0.423123] [G loss: 0.834550]\n",
            "[Epoch 0/1] [Batch 671/938] [D loss: 0.453699] [G loss: 2.065270]\n",
            "[Epoch 0/1] [Batch 672/938] [D loss: 0.617829] [G loss: 0.396546]\n",
            "[Epoch 0/1] [Batch 673/938] [D loss: 0.609429] [G loss: 2.768510]\n",
            "[Epoch 0/1] [Batch 674/938] [D loss: 0.508155] [G loss: 0.507957]\n",
            "[Epoch 0/1] [Batch 675/938] [D loss: 0.306875] [G loss: 1.699028]\n",
            "[Epoch 0/1] [Batch 676/938] [D loss: 0.310397] [G loss: 1.460402]\n",
            "[Epoch 0/1] [Batch 677/938] [D loss: 0.343322] [G loss: 0.929451]\n",
            "[Epoch 0/1] [Batch 678/938] [D loss: 0.340455] [G loss: 1.843732]\n",
            "[Epoch 0/1] [Batch 679/938] [D loss: 0.378746] [G loss: 0.847518]\n",
            "[Epoch 0/1] [Batch 680/938] [D loss: 0.427381] [G loss: 1.730387]\n",
            "[Epoch 0/1] [Batch 681/938] [D loss: 0.483419] [G loss: 0.592765]\n",
            "[Epoch 0/1] [Batch 682/938] [D loss: 0.477250] [G loss: 2.049877]\n",
            "[Epoch 0/1] [Batch 683/938] [D loss: 0.529796] [G loss: 0.511118]\n",
            "[Epoch 0/1] [Batch 684/938] [D loss: 0.468817] [G loss: 1.839221]\n",
            "[Epoch 0/1] [Batch 685/938] [D loss: 0.498464] [G loss: 0.570771]\n",
            "[Epoch 0/1] [Batch 686/938] [D loss: 0.397866] [G loss: 1.704936]\n",
            "[Epoch 0/1] [Batch 687/938] [D loss: 0.392037] [G loss: 0.841089]\n",
            "[Epoch 0/1] [Batch 688/938] [D loss: 0.325941] [G loss: 1.333626]\n",
            "[Epoch 0/1] [Batch 689/938] [D loss: 0.355431] [G loss: 1.224412]\n",
            "[Epoch 0/1] [Batch 690/938] [D loss: 0.435557] [G loss: 0.978928]\n",
            "[Epoch 0/1] [Batch 691/938] [D loss: 0.402433] [G loss: 1.040908]\n",
            "[Epoch 0/1] [Batch 692/938] [D loss: 0.369382] [G loss: 1.180167]\n",
            "[Epoch 0/1] [Batch 693/938] [D loss: 0.395895] [G loss: 1.103389]\n",
            "[Epoch 0/1] [Batch 694/938] [D loss: 0.351591] [G loss: 1.144939]\n",
            "[Epoch 0/1] [Batch 695/938] [D loss: 0.375395] [G loss: 1.336311]\n",
            "[Epoch 0/1] [Batch 696/938] [D loss: 0.426857] [G loss: 0.773492]\n",
            "[Epoch 0/1] [Batch 697/938] [D loss: 0.451123] [G loss: 1.825748]\n",
            "[Epoch 0/1] [Batch 698/938] [D loss: 0.585385] [G loss: 0.443430]\n",
            "[Epoch 0/1] [Batch 699/938] [D loss: 0.567787] [G loss: 2.805972]\n",
            "[Epoch 0/1] [Batch 700/938] [D loss: 0.538900] [G loss: 0.508838]\n",
            "[Epoch 0/1] [Batch 701/938] [D loss: 0.319534] [G loss: 1.640955]\n",
            "[Epoch 0/1] [Batch 702/938] [D loss: 0.286168] [G loss: 1.361347]\n",
            "[Epoch 0/1] [Batch 703/938] [D loss: 0.313473] [G loss: 1.231854]\n",
            "[Epoch 0/1] [Batch 704/938] [D loss: 0.341555] [G loss: 1.217504]\n",
            "[Epoch 0/1] [Batch 705/938] [D loss: 0.381727] [G loss: 1.127141]\n",
            "[Epoch 0/1] [Batch 706/938] [D loss: 0.388260] [G loss: 1.037811]\n",
            "[Epoch 0/1] [Batch 707/938] [D loss: 0.450370] [G loss: 1.304618]\n",
            "[Epoch 0/1] [Batch 708/938] [D loss: 0.538788] [G loss: 0.531123]\n",
            "[Epoch 0/1] [Batch 709/938] [D loss: 0.733797] [G loss: 2.783561]\n",
            "[Epoch 0/1] [Batch 710/938] [D loss: 0.960745] [G loss: 0.215023]\n",
            "[Epoch 0/1] [Batch 711/938] [D loss: 0.312725] [G loss: 1.342329]\n",
            "[Epoch 0/1] [Batch 712/938] [D loss: 0.464147] [G loss: 2.196178]\n",
            "[Epoch 0/1] [Batch 713/938] [D loss: 0.517003] [G loss: 0.581922]\n",
            "[Epoch 0/1] [Batch 714/938] [D loss: 0.336240] [G loss: 1.197125]\n",
            "[Epoch 0/1] [Batch 715/938] [D loss: 0.377508] [G loss: 1.543378]\n",
            "[Epoch 0/1] [Batch 716/938] [D loss: 0.429608] [G loss: 0.768606]\n",
            "[Epoch 0/1] [Batch 717/938] [D loss: 0.400083] [G loss: 1.272909]\n",
            "[Epoch 0/1] [Batch 718/938] [D loss: 0.473168] [G loss: 0.939588]\n",
            "[Epoch 0/1] [Batch 719/938] [D loss: 0.442270] [G loss: 0.837512]\n",
            "[Epoch 0/1] [Batch 720/938] [D loss: 0.436742] [G loss: 1.355419]\n",
            "[Epoch 0/1] [Batch 721/938] [D loss: 0.466534] [G loss: 0.666076]\n",
            "[Epoch 0/1] [Batch 722/938] [D loss: 0.460988] [G loss: 1.734344]\n",
            "[Epoch 0/1] [Batch 723/938] [D loss: 0.472853] [G loss: 0.662301]\n",
            "[Epoch 0/1] [Batch 724/938] [D loss: 0.402666] [G loss: 1.430712]\n",
            "[Epoch 0/1] [Batch 725/938] [D loss: 0.405252] [G loss: 0.937267]\n",
            "[Epoch 0/1] [Batch 726/938] [D loss: 0.358035] [G loss: 1.045356]\n",
            "[Epoch 0/1] [Batch 727/938] [D loss: 0.366327] [G loss: 1.332797]\n",
            "[Epoch 0/1] [Batch 728/938] [D loss: 0.424929] [G loss: 0.890305]\n",
            "[Epoch 0/1] [Batch 729/938] [D loss: 0.424781] [G loss: 1.245020]\n",
            "[Epoch 0/1] [Batch 730/938] [D loss: 0.447558] [G loss: 0.791642]\n",
            "[Epoch 0/1] [Batch 731/938] [D loss: 0.435536] [G loss: 1.523011]\n",
            "[Epoch 0/1] [Batch 732/938] [D loss: 0.539139] [G loss: 0.575819]\n",
            "[Epoch 0/1] [Batch 733/938] [D loss: 0.473290] [G loss: 1.813187]\n",
            "[Epoch 0/1] [Batch 734/938] [D loss: 0.590950] [G loss: 0.502182]\n",
            "[Epoch 0/1] [Batch 735/938] [D loss: 0.647403] [G loss: 1.765140]\n",
            "[Epoch 0/1] [Batch 736/938] [D loss: 0.780183] [G loss: 0.308433]\n",
            "[Epoch 0/1] [Batch 737/938] [D loss: 0.386688] [G loss: 1.713835]\n",
            "[Epoch 0/1] [Batch 738/938] [D loss: 0.365775] [G loss: 1.090440]\n",
            "[Epoch 0/1] [Batch 739/938] [D loss: 0.443367] [G loss: 0.858812]\n",
            "[Epoch 0/1] [Batch 740/938] [D loss: 0.483540] [G loss: 1.318953]\n",
            "[Epoch 0/1] [Batch 741/938] [D loss: 0.608145] [G loss: 0.533895]\n",
            "[Epoch 0/1] [Batch 742/938] [D loss: 0.569152] [G loss: 1.570833]\n",
            "[Epoch 0/1] [Batch 743/938] [D loss: 0.696818] [G loss: 0.395516]\n",
            "[Epoch 0/1] [Batch 744/938] [D loss: 0.564964] [G loss: 1.688355]\n",
            "[Epoch 0/1] [Batch 745/938] [D loss: 0.622828] [G loss: 0.469510]\n",
            "[Epoch 0/1] [Batch 746/938] [D loss: 0.453022] [G loss: 1.414866]\n",
            "[Epoch 0/1] [Batch 747/938] [D loss: 0.458675] [G loss: 0.869024]\n",
            "[Epoch 0/1] [Batch 748/938] [D loss: 0.435962] [G loss: 1.009632]\n",
            "[Epoch 0/1] [Batch 749/938] [D loss: 0.463208] [G loss: 1.109475]\n",
            "[Epoch 0/1] [Batch 750/938] [D loss: 0.492969] [G loss: 0.773864]\n",
            "[Epoch 0/1] [Batch 751/938] [D loss: 0.588416] [G loss: 1.262159]\n",
            "[Epoch 0/1] [Batch 752/938] [D loss: 0.727046] [G loss: 0.368526]\n",
            "[Epoch 0/1] [Batch 753/938] [D loss: 0.711091] [G loss: 1.924873]\n",
            "[Epoch 0/1] [Batch 754/938] [D loss: 0.640412] [G loss: 0.451921]\n",
            "[Epoch 0/1] [Batch 755/938] [D loss: 0.455725] [G loss: 1.140617]\n",
            "[Epoch 0/1] [Batch 756/938] [D loss: 0.413763] [G loss: 1.303033]\n",
            "[Epoch 0/1] [Batch 757/938] [D loss: 0.456720] [G loss: 0.814196]\n",
            "[Epoch 0/1] [Batch 758/938] [D loss: 0.421053] [G loss: 1.192402]\n",
            "[Epoch 0/1] [Batch 759/938] [D loss: 0.425474] [G loss: 1.017797]\n",
            "[Epoch 0/1] [Batch 760/938] [D loss: 0.456391] [G loss: 1.081310]\n",
            "[Epoch 0/1] [Batch 761/938] [D loss: 0.454493] [G loss: 0.888611]\n",
            "[Epoch 0/1] [Batch 762/938] [D loss: 0.532792] [G loss: 1.353113]\n",
            "[Epoch 0/1] [Batch 763/938] [D loss: 0.637185] [G loss: 0.438578]\n",
            "[Epoch 0/1] [Batch 764/938] [D loss: 0.577152] [G loss: 2.078035]\n",
            "[Epoch 0/1] [Batch 765/938] [D loss: 0.577136] [G loss: 0.549700]\n",
            "[Epoch 0/1] [Batch 766/938] [D loss: 0.426982] [G loss: 1.624816]\n",
            "[Epoch 0/1] [Batch 767/938] [D loss: 0.381330] [G loss: 1.071586]\n",
            "[Epoch 0/1] [Batch 768/938] [D loss: 0.370929] [G loss: 1.014664]\n",
            "[Epoch 0/1] [Batch 769/938] [D loss: 0.376541] [G loss: 1.501363]\n",
            "[Epoch 0/1] [Batch 770/938] [D loss: 0.426668] [G loss: 0.824284]\n",
            "[Epoch 0/1] [Batch 771/938] [D loss: 0.478868] [G loss: 1.560173]\n",
            "[Epoch 0/1] [Batch 772/938] [D loss: 0.563260] [G loss: 0.495184]\n",
            "[Epoch 0/1] [Batch 773/938] [D loss: 0.676136] [G loss: 2.433653]\n",
            "[Epoch 0/1] [Batch 774/938] [D loss: 0.692119] [G loss: 0.333517]\n",
            "[Epoch 0/1] [Batch 775/938] [D loss: 0.450383] [G loss: 1.731073]\n",
            "[Epoch 0/1] [Batch 776/938] [D loss: 0.378454] [G loss: 1.052824]\n",
            "[Epoch 0/1] [Batch 777/938] [D loss: 0.410068] [G loss: 1.000568]\n",
            "[Epoch 0/1] [Batch 778/938] [D loss: 0.409008] [G loss: 1.190228]\n",
            "[Epoch 0/1] [Batch 779/938] [D loss: 0.477021] [G loss: 0.859795]\n",
            "[Epoch 0/1] [Batch 780/938] [D loss: 0.479706] [G loss: 1.251719]\n",
            "[Epoch 0/1] [Batch 781/938] [D loss: 0.496965] [G loss: 0.723030]\n",
            "[Epoch 0/1] [Batch 782/938] [D loss: 0.473305] [G loss: 1.527097]\n",
            "[Epoch 0/1] [Batch 783/938] [D loss: 0.558553] [G loss: 0.620867]\n",
            "[Epoch 0/1] [Batch 784/938] [D loss: 0.537938] [G loss: 1.610835]\n",
            "[Epoch 0/1] [Batch 785/938] [D loss: 0.640924] [G loss: 0.391983]\n",
            "[Epoch 0/1] [Batch 786/938] [D loss: 0.562316] [G loss: 2.179324]\n",
            "[Epoch 0/1] [Batch 787/938] [D loss: 0.479763] [G loss: 0.697343]\n",
            "[Epoch 0/1] [Batch 788/938] [D loss: 0.357893] [G loss: 1.382307]\n",
            "[Epoch 0/1] [Batch 789/938] [D loss: 0.356935] [G loss: 1.365953]\n",
            "[Epoch 0/1] [Batch 790/938] [D loss: 0.406060] [G loss: 0.923813]\n",
            "[Epoch 0/1] [Batch 791/938] [D loss: 0.377930] [G loss: 1.345567]\n",
            "[Epoch 0/1] [Batch 792/938] [D loss: 0.387668] [G loss: 1.082294]\n",
            "[Epoch 0/1] [Batch 793/938] [D loss: 0.373462] [G loss: 1.089770]\n",
            "[Epoch 0/1] [Batch 794/938] [D loss: 0.409571] [G loss: 1.381827]\n",
            "[Epoch 0/1] [Batch 795/938] [D loss: 0.438233] [G loss: 0.727301]\n",
            "[Epoch 0/1] [Batch 796/938] [D loss: 0.495376] [G loss: 2.127305]\n",
            "[Epoch 0/1] [Batch 797/938] [D loss: 0.594826] [G loss: 0.466203]\n",
            "[Epoch 0/1] [Batch 798/938] [D loss: 0.469360] [G loss: 2.038739]\n",
            "[Epoch 0/1] [Batch 799/938] [D loss: 0.494398] [G loss: 0.636854]\n",
            "[Epoch 0/1] [Batch 800/938] [D loss: 0.429110] [G loss: 1.546884]\n",
            "[Epoch 0/1] [Batch 801/938] [D loss: 0.509898] [G loss: 0.626850]\n",
            "[Epoch 0/1] [Batch 802/938] [D loss: 0.459202] [G loss: 1.610849]\n",
            "[Epoch 0/1] [Batch 803/938] [D loss: 0.533508] [G loss: 0.594937]\n",
            "[Epoch 0/1] [Batch 804/938] [D loss: 0.550030] [G loss: 1.756816]\n",
            "[Epoch 0/1] [Batch 805/938] [D loss: 0.660051] [G loss: 0.404240]\n",
            "[Epoch 0/1] [Batch 806/938] [D loss: 0.467793] [G loss: 1.914061]\n",
            "[Epoch 0/1] [Batch 807/938] [D loss: 0.425411] [G loss: 0.790066]\n",
            "[Epoch 0/1] [Batch 808/938] [D loss: 0.365875] [G loss: 1.226012]\n",
            "[Epoch 0/1] [Batch 809/938] [D loss: 0.399787] [G loss: 1.162584]\n",
            "[Epoch 0/1] [Batch 810/938] [D loss: 0.427232] [G loss: 0.898930]\n",
            "[Epoch 0/1] [Batch 811/938] [D loss: 0.400019] [G loss: 1.424907]\n",
            "[Epoch 0/1] [Batch 812/938] [D loss: 0.463979] [G loss: 0.898054]\n",
            "[Epoch 0/1] [Batch 813/938] [D loss: 0.456657] [G loss: 1.262082]\n",
            "[Epoch 0/1] [Batch 814/938] [D loss: 0.453462] [G loss: 0.734427]\n",
            "[Epoch 0/1] [Batch 815/938] [D loss: 0.520824] [G loss: 1.749886]\n",
            "[Epoch 0/1] [Batch 816/938] [D loss: 0.669972] [G loss: 0.464662]\n",
            "[Epoch 0/1] [Batch 817/938] [D loss: 0.552115] [G loss: 1.783066]\n",
            "[Epoch 0/1] [Batch 818/938] [D loss: 0.536074] [G loss: 0.549430]\n",
            "[Epoch 0/1] [Batch 819/938] [D loss: 0.385624] [G loss: 1.478308]\n",
            "[Epoch 0/1] [Batch 820/938] [D loss: 0.437289] [G loss: 1.023669]\n",
            "[Epoch 0/1] [Batch 821/938] [D loss: 0.463293] [G loss: 0.729499]\n",
            "[Epoch 0/1] [Batch 822/938] [D loss: 0.540099] [G loss: 1.606349]\n",
            "[Epoch 0/1] [Batch 823/938] [D loss: 0.658915] [G loss: 0.385929]\n",
            "[Epoch 0/1] [Batch 824/938] [D loss: 0.522055] [G loss: 2.124008]\n",
            "[Epoch 0/1] [Batch 825/938] [D loss: 0.440649] [G loss: 0.685269]\n",
            "[Epoch 0/1] [Batch 826/938] [D loss: 0.306030] [G loss: 1.480818]\n",
            "[Epoch 0/1] [Batch 827/938] [D loss: 0.354453] [G loss: 1.442381]\n",
            "[Epoch 0/1] [Batch 828/938] [D loss: 0.435269] [G loss: 0.726256]\n",
            "[Epoch 0/1] [Batch 829/938] [D loss: 0.403129] [G loss: 1.615980]\n",
            "[Epoch 0/1] [Batch 830/938] [D loss: 0.447139] [G loss: 0.740793]\n",
            "[Epoch 0/1] [Batch 831/938] [D loss: 0.421214] [G loss: 1.422916]\n",
            "[Epoch 0/1] [Batch 832/938] [D loss: 0.467059] [G loss: 0.660266]\n",
            "[Epoch 0/1] [Batch 833/938] [D loss: 0.494081] [G loss: 1.750773]\n",
            "[Epoch 0/1] [Batch 834/938] [D loss: 0.644493] [G loss: 0.373357]\n",
            "[Epoch 0/1] [Batch 835/938] [D loss: 0.548096] [G loss: 2.150946]\n",
            "[Epoch 0/1] [Batch 836/938] [D loss: 0.496278] [G loss: 0.592005]\n",
            "[Epoch 0/1] [Batch 837/938] [D loss: 0.373680] [G loss: 1.317528]\n",
            "[Epoch 0/1] [Batch 838/938] [D loss: 0.372091] [G loss: 1.188840]\n",
            "[Epoch 0/1] [Batch 839/938] [D loss: 0.439268] [G loss: 0.901611]\n",
            "[Epoch 0/1] [Batch 840/938] [D loss: 0.435393] [G loss: 1.161582]\n",
            "[Epoch 0/1] [Batch 841/938] [D loss: 0.483343] [G loss: 0.816336]\n",
            "[Epoch 0/1] [Batch 842/938] [D loss: 0.471756] [G loss: 1.137811]\n",
            "[Epoch 0/1] [Batch 843/938] [D loss: 0.450171] [G loss: 0.756179]\n",
            "[Epoch 0/1] [Batch 844/938] [D loss: 0.553844] [G loss: 1.674333]\n",
            "[Epoch 0/1] [Batch 845/938] [D loss: 0.754373] [G loss: 0.293643]\n",
            "[Epoch 0/1] [Batch 846/938] [D loss: 0.595366] [G loss: 2.469978]\n",
            "[Epoch 0/1] [Batch 847/938] [D loss: 0.443012] [G loss: 0.688987]\n",
            "[Epoch 0/1] [Batch 848/938] [D loss: 0.328377] [G loss: 1.298414]\n",
            "[Epoch 0/1] [Batch 849/938] [D loss: 0.344196] [G loss: 1.463089]\n",
            "[Epoch 0/1] [Batch 850/938] [D loss: 0.381024] [G loss: 0.906345]\n",
            "[Epoch 0/1] [Batch 851/938] [D loss: 0.386789] [G loss: 1.478390]\n",
            "[Epoch 0/1] [Batch 852/938] [D loss: 0.424900] [G loss: 0.821514]\n",
            "[Epoch 0/1] [Batch 853/938] [D loss: 0.443373] [G loss: 1.515552]\n",
            "[Epoch 0/1] [Batch 854/938] [D loss: 0.545565] [G loss: 0.529710]\n",
            "[Epoch 0/1] [Batch 855/938] [D loss: 0.488117] [G loss: 2.184460]\n",
            "[Epoch 0/1] [Batch 856/938] [D loss: 0.478504] [G loss: 0.650526]\n",
            "[Epoch 0/1] [Batch 857/938] [D loss: 0.305245] [G loss: 1.538132]\n",
            "[Epoch 0/1] [Batch 858/938] [D loss: 0.317672] [G loss: 1.534362]\n",
            "[Epoch 0/1] [Batch 859/938] [D loss: 0.388165] [G loss: 0.848722]\n",
            "[Epoch 0/1] [Batch 860/938] [D loss: 0.394847] [G loss: 1.673342]\n",
            "[Epoch 0/1] [Batch 861/938] [D loss: 0.462120] [G loss: 0.643714]\n",
            "[Epoch 0/1] [Batch 862/938] [D loss: 0.479639] [G loss: 2.048618]\n",
            "[Epoch 0/1] [Batch 863/938] [D loss: 0.523492] [G loss: 0.523150]\n",
            "[Epoch 0/1] [Batch 864/938] [D loss: 0.435704] [G loss: 1.907091]\n",
            "[Epoch 0/1] [Batch 865/938] [D loss: 0.426931] [G loss: 0.756331]\n",
            "[Epoch 0/1] [Batch 866/938] [D loss: 0.373903] [G loss: 1.468171]\n",
            "[Epoch 0/1] [Batch 867/938] [D loss: 0.401436] [G loss: 0.893864]\n",
            "[Epoch 0/1] [Batch 868/938] [D loss: 0.301361] [G loss: 1.408082]\n",
            "[Epoch 0/1] [Batch 869/938] [D loss: 0.406192] [G loss: 1.390991]\n",
            "[Epoch 0/1] [Batch 870/938] [D loss: 0.492912] [G loss: 0.575852]\n",
            "[Epoch 0/1] [Batch 871/938] [D loss: 0.576967] [G loss: 2.719374]\n",
            "[Epoch 0/1] [Batch 872/938] [D loss: 0.656370] [G loss: 0.350057]\n",
            "[Epoch 0/1] [Batch 873/938] [D loss: 0.474758] [G loss: 2.296941]\n",
            "[Epoch 0/1] [Batch 874/938] [D loss: 0.397189] [G loss: 0.775464]\n",
            "[Epoch 0/1] [Batch 875/938] [D loss: 0.345746] [G loss: 1.474740]\n",
            "[Epoch 0/1] [Batch 876/938] [D loss: 0.312986] [G loss: 1.158447]\n",
            "[Epoch 0/1] [Batch 877/938] [D loss: 0.408662] [G loss: 1.440912]\n",
            "[Epoch 0/1] [Batch 878/938] [D loss: 0.457742] [G loss: 0.669239]\n",
            "[Epoch 0/1] [Batch 879/938] [D loss: 0.576503] [G loss: 2.393965]\n",
            "[Epoch 0/1] [Batch 880/938] [D loss: 0.710642] [G loss: 0.313676]\n",
            "[Epoch 0/1] [Batch 881/938] [D loss: 0.446116] [G loss: 2.472078]\n",
            "[Epoch 0/1] [Batch 882/938] [D loss: 0.341487] [G loss: 1.032464]\n",
            "[Epoch 0/1] [Batch 883/938] [D loss: 0.370690] [G loss: 0.981016]\n",
            "[Epoch 0/1] [Batch 884/938] [D loss: 0.395188] [G loss: 1.568754]\n",
            "[Epoch 0/1] [Batch 885/938] [D loss: 0.466106] [G loss: 0.654300]\n",
            "[Epoch 0/1] [Batch 886/938] [D loss: 0.507517] [G loss: 2.109718]\n",
            "[Epoch 0/1] [Batch 887/938] [D loss: 0.654888] [G loss: 0.376561]\n",
            "[Epoch 0/1] [Batch 888/938] [D loss: 0.637260] [G loss: 2.691868]\n",
            "[Epoch 0/1] [Batch 889/938] [D loss: 0.638295] [G loss: 0.438766]\n",
            "[Epoch 0/1] [Batch 890/938] [D loss: 0.365892] [G loss: 1.547519]\n",
            "[Epoch 0/1] [Batch 891/938] [D loss: 0.431989] [G loss: 1.147871]\n",
            "[Epoch 0/1] [Batch 892/938] [D loss: 0.450279] [G loss: 0.743426]\n",
            "[Epoch 0/1] [Batch 893/938] [D loss: 0.533565] [G loss: 1.809161]\n",
            "[Epoch 0/1] [Batch 894/938] [D loss: 0.640180] [G loss: 0.399862]\n",
            "[Epoch 0/1] [Batch 895/938] [D loss: 0.584151] [G loss: 2.171506]\n",
            "[Epoch 0/1] [Batch 896/938] [D loss: 0.554849] [G loss: 0.538351]\n",
            "[Epoch 0/1] [Batch 897/938] [D loss: 0.410027] [G loss: 1.410261]\n",
            "[Epoch 0/1] [Batch 898/938] [D loss: 0.438360] [G loss: 1.070225]\n",
            "[Epoch 0/1] [Batch 899/938] [D loss: 0.476282] [G loss: 0.873135]\n",
            "[Epoch 0/1] [Batch 900/938] [D loss: 0.420979] [G loss: 1.298847]\n",
            "[Epoch 0/1] [Batch 901/938] [D loss: 0.456137] [G loss: 0.777184]\n",
            "[Epoch 0/1] [Batch 902/938] [D loss: 0.456422] [G loss: 1.523927]\n",
            "[Epoch 0/1] [Batch 903/938] [D loss: 0.509670] [G loss: 0.598935]\n",
            "[Epoch 0/1] [Batch 904/938] [D loss: 0.523754] [G loss: 1.876085]\n",
            "[Epoch 0/1] [Batch 905/938] [D loss: 0.657928] [G loss: 0.397947]\n",
            "[Epoch 0/1] [Batch 906/938] [D loss: 0.595029] [G loss: 2.085616]\n",
            "[Epoch 0/1] [Batch 907/938] [D loss: 0.561353] [G loss: 0.472401]\n",
            "[Epoch 0/1] [Batch 908/938] [D loss: 0.490214] [G loss: 1.767329]\n",
            "[Epoch 0/1] [Batch 909/938] [D loss: 0.461362] [G loss: 0.703848]\n",
            "[Epoch 0/1] [Batch 910/938] [D loss: 0.441467] [G loss: 1.387890]\n",
            "[Epoch 0/1] [Batch 911/938] [D loss: 0.447796] [G loss: 0.841394]\n",
            "[Epoch 0/1] [Batch 912/938] [D loss: 0.439802] [G loss: 1.192785]\n",
            "[Epoch 0/1] [Batch 913/938] [D loss: 0.457207] [G loss: 0.889696]\n",
            "[Epoch 0/1] [Batch 914/938] [D loss: 0.453566] [G loss: 1.125049]\n",
            "[Epoch 0/1] [Batch 915/938] [D loss: 0.469219] [G loss: 0.874140]\n",
            "[Epoch 0/1] [Batch 916/938] [D loss: 0.451899] [G loss: 1.129654]\n",
            "[Epoch 0/1] [Batch 917/938] [D loss: 0.512315] [G loss: 0.764446]\n",
            "[Epoch 0/1] [Batch 918/938] [D loss: 0.491180] [G loss: 1.388044]\n",
            "[Epoch 0/1] [Batch 919/938] [D loss: 0.524310] [G loss: 0.594100]\n",
            "[Epoch 0/1] [Batch 920/938] [D loss: 0.611895] [G loss: 1.809826]\n",
            "[Epoch 0/1] [Batch 921/938] [D loss: 0.792951] [G loss: 0.286171]\n",
            "[Epoch 0/1] [Batch 922/938] [D loss: 0.555113] [G loss: 2.083863]\n",
            "[Epoch 0/1] [Batch 923/938] [D loss: 0.493492] [G loss: 0.647193]\n",
            "[Epoch 0/1] [Batch 924/938] [D loss: 0.377286] [G loss: 1.245052]\n",
            "[Epoch 0/1] [Batch 925/938] [D loss: 0.337857] [G loss: 1.373166]\n",
            "[Epoch 0/1] [Batch 926/938] [D loss: 0.394356] [G loss: 0.963763]\n",
            "[Epoch 0/1] [Batch 927/938] [D loss: 0.436655] [G loss: 1.115556]\n",
            "[Epoch 0/1] [Batch 928/938] [D loss: 0.454395] [G loss: 0.910007]\n",
            "[Epoch 0/1] [Batch 929/938] [D loss: 0.475515] [G loss: 1.175422]\n",
            "[Epoch 0/1] [Batch 930/938] [D loss: 0.486864] [G loss: 0.807761]\n",
            "[Epoch 0/1] [Batch 931/938] [D loss: 0.394981] [G loss: 1.375072]\n",
            "[Epoch 0/1] [Batch 932/938] [D loss: 0.447719] [G loss: 0.907285]\n",
            "[Epoch 0/1] [Batch 933/938] [D loss: 0.382189] [G loss: 1.172189]\n",
            "[Epoch 0/1] [Batch 934/938] [D loss: 0.408681] [G loss: 1.282878]\n",
            "[Epoch 0/1] [Batch 935/938] [D loss: 0.468616] [G loss: 0.722898]\n",
            "[Epoch 0/1] [Batch 936/938] [D loss: 0.502958] [G loss: 2.248285]\n",
            "[Epoch 0/1] [Batch 937/938] [D loss: 0.798896] [G loss: 0.254933]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5Rz9VH3TFBy"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6JRpM_FTE9r"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i_vg_yOTE5U"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-NWX0T2TE1R"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o72KRU-sTExX"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7NDtRptTEty"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvplqIprTEn0"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}